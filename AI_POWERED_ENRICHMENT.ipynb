{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "7c5qmmnaxy2lgvrppilq",
   "authorId": "3129431667105",
   "authorName": "FUKAMORI",
   "authorEmail": "kenshiro.fukamori@snowflake.com",
   "sessionId": "31581995-c62e-42b0-9c25-ed0b21934a52",
   "lastEditTime": 1765922102019
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530bdf82-fdd6-4e81-95ef-9adfdca85818",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "-- plotlyã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
  },
  {
   "cell_type": "code",
   "id": "07ce6bab-3ee2-400f-9ceb-f053413dc7a0",
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": "alter session set use_cached_result = false;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# setup for AI-powered enrichment\n# Import required libraries (available in Snowflake notebooks)\nimport json\nimport re\nimport pandas as pd\nfrom typing import List, Dict, Any\nfrom snowflake.snowpark import Session\n\n# Get the built-in Snowpark session\nsession = get_active_session()\n\n# Configuration\nHOURS_BACK = 12  # How many hours back to look in query history\nSEMANTIC_VIEW_NAME = 'HR_SEMANTIC_VIEW'\nCORTEX_MODEL = 'claude-4-sonnet'  # Claude model with high token limit\n\n# Set context for the analysis\nsession.sql(\"USE ROLE agentic_analytics_vhol_role\").collect()\nsession.sql(\"USE DATABASE SV_VHOL_DB\").collect()\nsession.sql(\"USE SCHEMA VHOL_SCHEMA\").collect()\n\n# Verify connection\ncurrent_context = session.sql(\"\"\"\n    SELECT \n        CURRENT_DATABASE() as database,\n        CURRENT_SCHEMA() as schema,\n        CURRENT_WAREHOUSE() as warehouse,\n        CURRENT_ROLE() as role,\n        CURRENT_USER() as user\n\"\"\").collect()\n\ncurrent_context",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "# Query to retrieve VHOL Seed Queries from history\nquery_alter_timezone = f\"\"\" ALTER SESSION SET TIMEZONE = 'Asia/Tokyo' \"\"\"\n\nquery_history_sql = f\"\"\"\nSELECT \n    QUERY_TEXT,\n    START_TIME,\n    EXECUTION_STATUS,\n    USER_NAME\nFROM \n    SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\nWHERE \n    START_TIME >= DATEADD('hour', -{HOURS_BACK}, CURRENT_TIMESTAMP())\n    AND QUERY_TEXT ILIKE '%VHOL_Seed_Query%'\n    AND QUERY_TEXT NOT ILIKE '%QUERY_TEXT%'\n    AND EXECUTION_STATUS = 'SUCCESS'\nORDER BY \n    START_TIME DESC\nLIMIT 50\n\"\"\"\n\nprint(f\"ðŸ” Retrieving VHOL Seed Queries from last {HOURS_BACK} hours...\")\n\n# Execute query and convert to pandas DataFrame\nquery_history_result = session.sql(query_alter_timezone)\nquery_history_result = session.sql(query_history_sql).collect()\nquery_history_df = pd.DataFrame([dict(row.asDict()) for row in query_history_result])\n\nprint(f\"ðŸ“Š Found {len(query_history_df)} VHOL Seed Queries in the last {HOURS_BACK} hours\")\n\nif len(query_history_df) > 0:\n    print(\"\\nSample queries found:\")\n    for i, row in query_history_df.head(3).iterrows():\n        print(f\"\\n{i+1}. Query at {row['START_TIME']}:\")\n        # Show first 1000 characters of query\n        query_preview = row['QUERY_TEXT'][:1000] + \"...\" if len(row['QUERY_TEXT']) > 1000 else row['QUERY_TEXT']\n        print(f\"   {query_preview}\")\nelse:\n    print(\"âš ï¸  No VHOL Seed Queries found. You may need to:\")\n    print(\"   1. Run some queries with 'VHOL Seed Query' comments\")\n    print(\"   2. Increase the HOURS_BACK parameter\")\n    print(\"   3. Check that the queries executed successfully\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d2f06081-c0d8-4a07-8b4b-26d946a97eb1",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "ALTER SESSION SET TIMEZONE = 'Asia/Tokyo';\n\nSELECT \n    QUERY_TEXT,\n    START_TIME,\n    EXECUTION_STATUS,\n    USER_NAME\nFROM \n    SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\nORDER BY START_TIME desc\nLIMIT 100\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cca48a71-5aa4-40a4-b683-ee8323c39bda",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "ALTER SESSION SET TIMEZONE = 'Asia/Tokyo';\n\nSELECT \n    QUERY_TEXT,\n    START_TIME,\n    EXECUTION_STATUS,\n    USER_NAME\nFROM \n    SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\nWHERE \n    START_TIME >= DATEADD('hour', -12, CURRENT_TIMESTAMP())\n    AND QUERY_TEXT ILIKE '%vhol_seed_query%'\n    AND QUERY_TEXT NOT ILIKE '%QUERY_TEXT%'\n    AND EXECUTION_STATUS = 'SUCCESS'\nORDER BY \n    START_TIME DESC\nLIMIT 50",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33d91ee6-0312-41b5-86f2-2fc97c7c58f6",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "def extract_metrics_and_dimensions(query_text: str) -> Dict[str, List[str]]:\n    \"\"\"\n    Extract metrics (aggregation functions) and dimensions from SQL query\n    \"\"\"\n    metrics = []\n    dimensions = []\n    \n    # Clean query text\n    query_clean = re.sub(r'--.*?\\n', '\\n', query_text)  # Remove line comments\n    query_clean = re.sub(r'/\\*.*?\\*/', '', query_clean, flags=re.DOTALL)  # Remove block comments\n    query_upper = query_clean.upper()\n    \n    # Extract aggregation functions (metrics)\n    metric_patterns = [\n        r'COUNT\\s*\\([^)]+\\)',\n        r'SUM\\s*\\([^)]+\\)',\n        r'AVG\\s*\\([^)]+\\)',\n        r'MIN\\s*\\([^)]+\\)',\n        r'MAX\\s*\\([^)]+\\)',\n        r'STDDEV\\s*\\([^)]+\\)',\n        r'PERCENTILE_CONT\\s*\\([^)]+\\)',\n        r'ROUND\\s*\\([^)]+\\)',\n    ]\n    \n    for pattern in metric_patterns:\n        matches = re.findall(pattern, query_upper)\n        metrics.extend(matches)\n    \n    # Extract column references from SELECT, WHERE, GROUP BY\n    column_patterns = [\n        r'SELECT\\s+.*?([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)',  # table.column in SELECT\n        r'WHERE\\s+.*?([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)',   # table.column in WHERE\n        r'GROUP BY\\s+.*?([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)', # table.column in GROUP BY\n        r'EXTRACT\\s*\\(\\s*[A-Z]+\\s+FROM\\s+([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)\\)',  # EXTRACT functions\n        r'DATEDIFF\\s*\\([^,]+,\\s*([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)',  # DATEDIFF functions\n    ]\n    \n    for pattern in column_patterns:\n        matches = re.findall(pattern, query_upper)\n        for match in matches:\n            # Skip if it's part of an aggregation function\n            if not any(agg in match for agg in ['COUNT', 'SUM', 'AVG', 'MIN', 'MAX']):\n                dimensions.append(match)\n    \n    # Clean and deduplicate\n    metrics = list(set([m.strip() for m in metrics if m.strip()]))\n    dimensions = list(set([d.strip() for d in dimensions if d.strip()]))\n    \n    return {\n        'metrics': metrics,\n        'dimensions': dimensions\n    }\n\n# Analyze all queries\nall_metrics = []\nall_dimensions = []\n\nprint(\"ðŸ” Analyzing queries for metrics and dimensions...\")\n\nfor i, row in query_history_df.iterrows():\n    analysis = extract_metrics_and_dimensions(row['QUERY_TEXT'])\n    all_metrics.extend(analysis['metrics'])\n    all_dimensions.extend(analysis['dimensions'])\n\n# Deduplicate and summarize\nunique_metrics = list(set(all_metrics))\nunique_dimensions = list(set(all_dimensions))\n\nprint(f\"\\nðŸ“ˆ Analysis Results (with aliases):\")\nprint(f\"   Total unique metrics found: {len(unique_metrics)}\")\nprint(f\"   Total unique dimensions found: {len(unique_dimensions)}\")\n\nif unique_metrics:\n    print(f\"\\nðŸ”¢ Sample Metrics (with aliases):\")\n    for metric in unique_metrics[:5]:  # Show first 5\n        print(f\"   - {metric}\")\n\nif unique_dimensions:\n    print(f\"\\nðŸ“Š Sample Dimensions (with aliases):\")\n    for dim in unique_dimensions[:5]:  # Show first 5\n        print(f\"   - {dim}\")\n\n# VHOL table alias mappings\nalias_to_table = {\n    'F': 'HR_EMPLOYEE_FACT',\n    'E': 'EMPLOYEE_DIM', \n    'D': 'DEPARTMENT_DIM',\n    'J': 'JOB_DIM',\n    'L': 'LOCATION_DIM'\n}\n\nprint(f\"\\nðŸ”§ Resolving VHOL table aliases to actual table names...\")\nprint(f\"ðŸ“‹ Alias mappings: {alias_to_table}\")\n\n# Resolve aliases in metrics\nresolved_metrics = []\nfor metric in unique_metrics:\n    resolved_metric = metric\n    for alias, table in alias_to_table.items():\n        resolved_metric = resolved_metric.replace(f'{alias}.', f'{table}.')\n    resolved_metrics.append(resolved_metric)\n\n# Resolve aliases in dimensions\nresolved_dimensions = []\nfor dim in unique_dimensions:\n    if '.' in dim:\n        table_alias = dim.split('.')[0]\n        column_name = dim.split('.')[1]\n        \n        if table_alias in alias_to_table:\n            resolved_dim = f\"{alias_to_table[table_alias]}.{column_name}\"\n            resolved_dimensions.append(resolved_dim)\n        else:\n            resolved_dimensions.append(dim)\n    else:\n        resolved_dimensions.append(dim)\n\n# Update with resolved names\nunique_metrics = list(set(resolved_metrics))\nunique_dimensions = list(set(resolved_dimensions))\n\nprint(f\"\\nâœ… Final Analysis Results (aliases resolved):\")\nprint(f\"   ðŸ“Š Resolved metrics: {len(unique_metrics)}\")\nprint(f\"   ðŸ“ Resolved dimensions: {len(unique_dimensions)}\")\n\nif unique_metrics:\n    print(f\"\\nðŸ”¢ Final Resolved Metrics:\")\n    for metric in unique_metrics[:5]:\n        print(f\"   - {metric}\")\n\nif unique_dimensions:\n    print(f\"\\nðŸ“Š Final Resolved Dimensions:\")\n    for dim in unique_dimensions[:5]:\n        print(f\"   - {dim}\")\n\nprint(f\"\\nðŸŽ¯ Ready for semantic view enhancement!\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d42480f9-5eb7-46c2-af72-6bc8cc664f9a",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Retrieve current semantic view DDL\nprint(f\"ðŸ“‹ Retrieving DDL for {SEMANTIC_VIEW_NAME}...\")\n\ntry:\n    ddl_result = session.sql(f\"SELECT GET_DDL('semantic_view','{SEMANTIC_VIEW_NAME}') as DDL\").collect()\n    \n    if ddl_result and len(ddl_result) > 0:\n        current_ddl = ddl_result[0]['DDL']\n        print(f\"âœ… Retrieved DDL for {SEMANTIC_VIEW_NAME}\")\n        print(f\"ðŸ“ DDL Length: {len(current_ddl)} characters\")\n        \n        # Show first few lines\n        ddl_lines = current_ddl.split('\\n')\n        print(f\"\\nðŸ“‹ Preview (first 20 lines):\")\n        for i, line in enumerate(ddl_lines[:20]):\n            print(f\"   {i+1:2d}: {line}\")\n        \n        if len(ddl_lines) > 20:\n            print(f\"   ... ({len(ddl_lines)-20} more lines)\")\n    else:\n        print(f\"âŒ No DDL found for {SEMANTIC_VIEW_NAME}\")\n        current_ddl = \"\"\n        \nexcept Exception as e:\n    print(f\"âŒ Error retrieving DDL: {e}\")\n    current_ddl = \"\"\n\nif current_ddl:\n    print(f\"\\nâœ… DDL retrieval successful! Ready for AI enhancement.\")\nelse:\n    print(f\"\\nâš ï¸  No DDL available - you may need to create the semantic view first.\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aaaf25a3-e1b2-4953-bf3e-14beb3799c9f",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "if current_ddl and (unique_metrics or unique_dimensions):\n    # Create AI prompt for enhancement (optimized for token efficiency)\n    top_metrics = unique_metrics[:10]  # Top 10 most important\n    top_dimensions = unique_dimensions[:10]  # Top 10 most important\n    \n    prompt = f\"\"\"\næ¬¡ã® CREATE SEMANTIC VIEW ã® DDL ã‚’ã€æ¤œå‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦\næ–°ã—ã„ METRICS / DIMENSIONS ã®å®šç¾©ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§æ‹¡å¼µã—ã¦ãã ã•ã„ã€‚\n\nã€ç¾åœ¨ã® DDLã€‘\n{current_ddl}\n\nã€è¿½åŠ ã—ãŸã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘\n{', '.join(top_metrics)}\n\nã€è¿½åŠ ã—ãŸã„ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã€‘\n{', '.join(top_dimensions)}\n\nã€ãƒ«ãƒ¼ãƒ«ã€‘\n- æ—¢å­˜ã®å†…å®¹ã¯ä¸€åˆ‡å¤‰æ›´ã›ãšã€ãã®ã¾ã¾æ®‹ã—ã¦ãã ã•ã„\n- é‡è¦: DDL å†…ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³é †åºã¯å¿…ãš FACTS(), DIMENSIONS(), METRICS() ã®é †ã«ä¿ã£ã¦ãã ã•ã„\n- ã™ã¹ã¦ã®é›†è¨ˆå¼ï¼ˆSUM, COUNT, AVG ãªã©ï¼‰ã¯ METRICS() ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã«è¿½åŠ ã—ã¦ãã ã•ã„\n- METRICS() ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã®æ›¸å¼ã¯\n    table_name.metric_name AS AGG(expression) --- added with AI enhancement\n  ã¨ã—ã¦ãã ã•ã„\n- FACTS() ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ãƒ†ãƒ¼ãƒ–ãƒ«å‚ç…§å°‚ç”¨ã§ã‚ã‚Šã€é›†è¨ˆå¼ã¯å«ã‚ãªã„ã§ãã ã•ã„\n- é›†è¨ˆã‚’ä¼´ã‚ãªã„åˆ—å‚ç…§ã¯ DIMENSIONS ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ ã—ã¦ãã ã•ã„\n- å‡ºåŠ›ã«ã¯ \"WITH EXTENSION\" ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å«ã‚ãªã„ã§ãã ã•ã„\n- è¿½åŠ ã—ãŸè¡Œã«ã¯å¿…ãš\n    --- added with AI enhancement\n  ã¨ã„ã†ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä»˜ã‘ã¦ãã ã•ã„\n- è§£èª¬æ–‡ã‚„èª¬æ˜Žæ–‡ã¯ä¸€åˆ‡å‡ºåŠ›ã›ãšã€ã€Œå®Œæˆã—ãŸæ‹¡å¼µå¾Œã® DDL å…¨ä½“ã®ã¿ã€ã‚’è¿”ã—ã¦ãã ã•ã„\n- å§‹ã¾ã‚Šã®ã€Œ``` sqlã€ã‚„çµ‚ã‚ã‚Šã®ã€Œ``` ã€ã¯ä¸è¦ã§ã™ã€‚\n\nã€æ­£ã—ã„ DDL æ§‹é€ ã®ä¾‹ã€‘\nFACTS (table_references)\nDIMENSIONS (column_references)\nMETRICS (\nHR_EMPLOYEE_FACT.total_salary AS SUM(salary) --- added with AI enhancement\n)\n\nã€å‡ºåŠ›ã€‘\næ‹¡å¼µå¾Œã® CREATE SEMANTIC VIEW DDL å…¨ä½“ã‚’ãã®ã¾ã¾å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\"\"\"\n    \n    # Escape single quotes for SQL\n    prompt_escaped = prompt.replace(\"'\", \"''\")\n    \n    # Use CORTEX_COMPLETE to generate enhanced DDL\n    cortex_sql = f\"\"\"\n    SELECT SNOWFLAKE.CORTEX.COMPLETE(\n        '{CORTEX_MODEL}',\n        '{prompt_escaped}'\n    ) as enhanced_ddl\n    \"\"\"\n    \n    print(f\"ðŸ¤– Using CORTEX_COMPLETE with {CORTEX_MODEL} to enhance semantic view...\")\n    print(\"   This may take 30-60 seconds...\")\n    \n    try:\n        # Execute CORTEX_COMPLETE\n        cortex_result = session.sql(cortex_sql).collect()\n        \n        if cortex_result and len(cortex_result) > 0:\n            enhanced_ddl = cortex_result[0]['ENHANCED_DDL']\n            print(\"\\nâœ… Successfully generated enhanced semantic view DDL!\")\n            \n            # Show statistics\n            original_lines = len(current_ddl.split('\\n'))\n            enhanced_lines = len(enhanced_ddl.split('\\n'))\n            \n            print(f\"ðŸ“Š Enhancement Statistics:\")\n            print(f\"   Original DDL: {original_lines} lines, {len(current_ddl)} characters\")\n            print(f\"   Enhanced DDL: {enhanced_lines} lines, {len(enhanced_ddl)} characters\")\n            print(f\"   Lines added: {enhanced_lines - original_lines}\")\n            \n            # Count new metrics and dimensions by looking for AI enhancement comments\n            ai_additions_count = enhanced_ddl.count('--- added with AI enhancement')\n            \n            print(f\"   New metrics/dimensions added: {ai_additions_count}\")\n            \n        else:\n            print(\"âŒ CORTEX_COMPLETE returned no result\")\n            enhanced_ddl = current_ddl\n            \n    except Exception as e:\n        print(f\"âŒ Error with CORTEX_COMPLETE: {e}\")\n        enhanced_ddl = current_ddl\n        \nelse:\n    print(\"âš ï¸  Skipping enhancement - no DDL or no new metrics/dimensions found\")\n    enhanced_ddl = current_ddl if 'current_ddl' in locals() else \"\"\n\n# Display enhanced DDL results\nif 'enhanced_ddl' in locals() and enhanced_ddl:\n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPLETE ENHANCED SEMANTIC VIEW DDL\")\n    print(\"=\"*80)\n    print(\"ðŸ“ COMPLETE DDL OUTPUT (no truncation):\")\n    print()\n    print(enhanced_ddl)\n    print()\n    print(\"=\"*80)\n    \n    # Highlight the new AI-enhanced additions\n    enhanced_lines = enhanced_ddl.split('\\n')\n    new_additions = [line for line in enhanced_lines if '--- added with AI enhancement' in line]\n    \n    if new_additions:\n        print(\"\\nðŸ¤– AI-ENHANCED ADDITIONS:\")\n        print(\"-\" * 50)\n        for addition in new_additions:\n            print(addition.strip())\n    else:\n        print(\"\\nâš ï¸  No new additions detected in the enhanced DDL\")\n    \n    print(f\"\\nðŸ’¡ Next Steps:\")\n    print(f\"   1. Review the enhanced DDL above\")\n    print(f\"   2. Test the DDL in a development environment\")\n    print(f\"   3. Deploy to production when ready\")\n    print(f\"   4. Update documentation with new metrics/dimensions\")\n    \nelse:\n    print(\"âŒ No enhanced DDL available\")\n\nprint(\"\\nðŸŽ‰ Analysis complete!\")\nif 'query_history_df' in locals():\n    print(f\"   â€¢ Analyzed {len(query_history_df)} queries from the last {HOURS_BACK} hours\")\nif 'unique_metrics' in locals():\n    print(f\"   â€¢ Found {len(unique_metrics)} unique metrics\")\nif 'unique_dimensions' in locals():\n    print(f\"   â€¢ Found {len(unique_dimensions)} unique dimensions\")\nprint(f\"   â€¢ Enhanced {SEMANTIC_VIEW_NAME} using {CORTEX_MODEL}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4905fbac-294d-4a9b-9cc5-6997d9416f87",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "///ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—SQL\ncreate or replace semantic view HR_SEMANTIC_VIEW  \n\ttables (  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.DEPARTMENT_DIM primary key (DEPARTMENT_KEY) comment='ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯ã€çµ„ç¹”ã®éƒ¨é–€ã®è¨˜éŒ²ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯è­˜åˆ¥æƒ…å ±ã‚’æŒã¤1ã¤ã®éƒ¨é–€ã‚’è¡¨ã—ã¾ã™ã€‚',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.EMPLOYEE_DIM primary key (EMPLOYEE_KEY) comment='ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯ã€åŸºæœ¬çš„ãªäººå£çµ±è¨ˆå­¦çš„æƒ…å ±ã¨é›‡ç”¨æƒ…å ±ã‚’å«ã‚€å¾“æ¥­å“¡ã®è¨˜éŒ²ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯1äººã®å¾“æ¥­å“¡ã‚’è¡¨ã—ã€å€‹äººæƒ…å ±ã¨æŽ¡ç”¨æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.HR_EMPLOYEE_FACT primary key (HR_FACT_ID) comment='ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯ã€åˆ†æžç›®çš„ã®ãŸã‚ã«ãƒ•ã‚¡ã‚¯ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦æ•´ç†ã•ã‚ŒãŸå¾“æ¥­å“¡æƒ…å ±ã®è¨˜éŒ²ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ç‰¹å®šã®æ™‚ç‚¹ã§ã®å¾“æ¥­å“¡ã‚’è¡¨ã—ã€éƒ¨é–€ã€è·å‹™ã€å ´æ‰€ã®æ¬¡å…ƒå‚ç…§ã¨çµ¦ä¸ŽãŠã‚ˆã³é›¢è·çŠ¶æ³ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.JOB_DIM primary key (JOB_KEY) comment='ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯ã€çµ„ç¹”å†…ã®è·ä½ã®è¨˜éŒ²ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯å€‹åˆ¥ã®è·å‹™ã‚’è¡¨ã—ã€è·ä½ã®åç§°ã¨éšŽå±¤ãƒ¬ãƒ™ãƒ«ã«é–¢ã™ã‚‹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.LOCATION_DIM primary key (LOCATION_KEY) comment='ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯ã€æ¬¡å…ƒãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®å‚ç…§ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹å ´æ‰€ã®è¨˜éŒ²ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯è­˜åˆ¥æƒ…å ±ã‚’æŒã¤å€‹åˆ¥ã®å ´æ‰€ã‚’è¡¨ã—ã¾ã™ã€‚'  \n\t)  \n\trelationships (  \n\t\tHR_EMPLOYEE_FACT_TO_DEPARTMENT_DIM as HR_EMPLOYEE_FACT(DEPARTMENT_KEY) references DEPARTMENT_DIM(DEPARTMENT_KEY),  \n\t\tHR_EMPLOYEE_FACT_TO_EMPLOYEE_DIM as HR_EMPLOYEE_FACT(EMPLOYEE_KEY) references EMPLOYEE_DIM(EMPLOYEE_KEY),  \n\t\tHR_EMPLOYEE_FACT_TO_JOB_DIM as HR_EMPLOYEE_FACT(JOB_KEY) references JOB_DIM(JOB_KEY),  \n\t\tHR_EMPLOYEE_FACT_TO_LOCATION_DIM as HR_EMPLOYEE_FACT(LOCATION_KEY) references LOCATION_DIM(LOCATION_KEY)  \n\t)  \n\tfacts (  \n\t\tHR_EMPLOYEE_FACT.SALARY as SALARY comment='å¾“æ¥­å“¡ã®çµ¦ä¸Žé¡ã€‚'  \n\t)  \n\tdimensions (  \n\t\tDEPARTMENT_DIM.DEPARTMENT_KEY as DEPARTMENT_KEY comment='æ¬¡å…ƒãƒ†ãƒ¼ãƒ–ãƒ«ã®å„éƒ¨é–€ã®ä¸€æ„ã®æ•°å€¤è­˜åˆ¥å­ã€‚',  \n\t\tDEPARTMENT_DIM.DEPARTMENT_NAME as DEPARTMENT_NAME comment='çµ„ç¹”å†…ã®éƒ¨é–€ã®åç§°ã€‚',  \n\t\tEMPLOYEE_DIM.EMPLOYEE_KEY as EMPLOYEE_KEY comment='æ¬¡å…ƒãƒ†ãƒ¼ãƒ–ãƒ«ã®å„å¾“æ¥­å“¡ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€æ„ã®æ•°å€¤è­˜åˆ¥å­ã€‚',  \n\t\tEMPLOYEE_DIM.EMPLOYEE_NAME as EMPLOYEE_NAME comment='å¾“æ¥­å“¡ã®ãƒ•ãƒ«ãƒãƒ¼ãƒ ã€‚',  \n\t\tEMPLOYEE_DIM.GENDER as GENDER comment='å¾“æ¥­å“¡ã®æ€§åˆ¥åŒºåˆ†ã€‚',  \n\t\tEMPLOYEE_DIM.HIRE_DATE as HIRE_DATE comment='å¾“æ¥­å“¡ãŒçµ„ç¹”ã«æŽ¡ç”¨ã•ã‚ŒãŸæ—¥ä»˜ã€‚',  \n\t\tHR_EMPLOYEE_FACT.ATTRITION_FLAG as ATTRITION_FLAG comment='å¾“æ¥­å“¡ãŒçµ„ç¹”ã‚’é›¢ã‚ŒãŸã‹ã©ã†ã‹ã‚’ç¤ºã™ãƒ•ãƒ©ã‚°ã€‚',  \n\t\tHR_EMPLOYEE_FACT.DEPARTMENT_KEY as DEPARTMENT_KEY comment='çµ„ç¹”å†…ã®éƒ¨é–€ã®ä¸€æ„ã®è­˜åˆ¥å­ã€‚',  \n\t\tHR_EMPLOYEE_FACT.EMPLOYEE_KEY as EMPLOYEE_KEY comment='ã‚·ã‚¹ãƒ†ãƒ å†…ã®å„å¾“æ¥­å“¡ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€æ„ã®è­˜åˆ¥å­ã€‚',  \n\t\tHR_EMPLOYEE_FACT.HR_FACT_ID as HR_FACT_ID comment='å„äººäº‹ãƒ•ã‚¡ã‚¯ãƒˆãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€æ„ã®è­˜åˆ¥å­ã€‚',  \n\t\tHR_EMPLOYEE_FACT.JOB_KEY as JOB_KEY comment='çµ„ç¹”å†…ã®è·ä½ã®ä¸€æ„ã®è­˜åˆ¥å­ã€‚',  \n\t\tHR_EMPLOYEE_FACT.LOCATION_KEY as LOCATION_KEY comment='å¾“æ¥­å“¡ã®å‹¤å‹™åœ°ã®ä¸€æ„ã®è­˜åˆ¥å­ã€‚',  \n\t\tHR_EMPLOYEE_FACT.DATE as DATE comment='DATEåž‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹åˆ—ã€‚',  \n\t\tJOB_DIM.JOB_KEY as JOB_KEY comment='æ¬¡å…ƒãƒ†ãƒ¼ãƒ–ãƒ«ã®å„è·å‹™ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€æ„ã®è­˜åˆ¥å­ã€‚',  \n\t\tJOB_DIM.JOB_TITLE as JOB_TITLE comment='å¾“æ¥­å“¡ãŒä¿æŒã™ã‚‹è·ä½ã¾ãŸã¯å½¹è·ã€‚',  \n\t\tLOCATION_DIM.LOCATION_KEY as LOCATION_KEY comment='å„å ´æ‰€ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€æ„ã®æ•°å€¤è­˜åˆ¥å­ã€‚',  \n\t\tLOCATION_DIM.LOCATION_NAME as LOCATION_NAME comment='éƒ½å¸‚ã¨ãã‚Œã«å¯¾å¿œã™ã‚‹å·žã®åç§°ã€‚',  \n\t\tJOB_DIM.JOB_LEVEL as JOB_LEVEL comment='è·å‹™ã®éšŽå±¤ãƒ¬ãƒ™ãƒ«ã€‚' --- added with AI enhancement  \n\t)  \n\tmetrics (  \n\t\tHR_EMPLOYEE_FACT.female_employee_count AS COUNT(DISTINCT CASE WHEN EMPLOYEE_DIM.GENDER = 'F' THEN HR_EMPLOYEE_FACT.EMPLOYEE_KEY END) comment='å¥³æ€§å¾“æ¥­å“¡æ•°ã€‚', --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.total_employee_count AS COUNT(DISTINCT HR_EMPLOYEE_FACT.EMPLOYEE_KEY) comment='ç·å¾“æ¥­å“¡æ•°ã€‚', --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.min_salary AS MIN(HR_EMPLOYEE_FACT.SALARY) comment='æœ€ä½Žçµ¦ä¸Žé¡ã€‚', --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.min_date AS MIN(HR_EMPLOYEE_FACT.DATE) comment='æœ€å°æ—¥ä»˜ã€‚', --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.max_date AS MAX(HR_EMPLOYEE_FACT.DATE) comment='æœ€å¤§æ—¥ä»˜ã€‚', --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.total_attrition AS SUM(HR_EMPLOYEE_FACT.ATTRITION_FLAG) comment='ç·é›¢è·è€…æ•°ã€‚', --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.salary_stddev AS STDDEV(HR_EMPLOYEE_FACT.SALARY) comment='çµ¦ä¸Žã®æ¨™æº–åå·®ã€‚', --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.total_records AS COUNT(*) comment='ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã€‚', --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.avg_tenure_days AS AVG(DATEDIFF('DAY', EMPLOYEE_DIM.HIRE_DATE, HR_EMPLOYEE_FACT.DATE)) comment='å¹³å‡åœ¨è·æ—¥æ•°ã€‚', --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.distinct_department_count AS COUNT(DISTINCT HR_EMPLOYEE_FACT.DEPARTMENT_KEY) comment='ç•°ãªã‚‹éƒ¨é–€æ•°ã€‚' --- added with AI enhancement  \n\t)  ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe19254a-5556-4374-bc56-9d979153e351",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Deploy the enhanced semantic view DDL\nif 'enhanced_ddl' in locals() and enhanced_ddl and enhanced_ddl.strip():\n    print(\"ðŸš€ Deploying Enhanced Semantic View...\")\n    print(\"=\"*60)\n    \n    try:\n        # First, drop the existing semantic view if it exists\n        drop_sql = f\"DROP SEMANTIC VIEW IF EXISTS {SEMANTIC_VIEW_NAME}\"\n        print(f\"ðŸ“‹ Dropping existing semantic view: {SEMANTIC_VIEW_NAME}\")\n        session.sql(drop_sql).collect()\n        print(\"   âœ… Existing semantic view dropped successfully\")\n        \n        # Execute the enhanced DDL\n        print(f\"ðŸ”§ Creating enhanced semantic view...\")\n        session.sql(enhanced_ddl).collect()\n        print(\"   âœ… Enhanced semantic view created successfully!\")\n        \n        # Verify the deployment\n        verification_sql = f\"SHOW SEMANTIC VIEWS LIKE '{SEMANTIC_VIEW_NAME}'\"\n        result = session.sql(verification_sql).collect()\n        \n        if result:\n            print(f\"\\nðŸŽ‰ SUCCESS! Enhanced {SEMANTIC_VIEW_NAME} deployed successfully!\")\n            print(f\"ðŸ“Š Semantic view details:\")\n            for row in result:\n                print(f\"   Name: {row['name']}\")\n                print(f\"   Database: {row['database_name']}\")\n                print(f\"   Schema: {row['schema_name']}\")\n                print(f\"   Created: {row['created_on']}\")\n        else:\n            print(f\"âš ï¸  Deployment completed but verification failed - please check manually\")\n            \n        # Show what was added\n        if '--- added with AI enhancement' in enhanced_ddl:\n            additions_count = enhanced_ddl.count('--- added with AI enhancement')\n            print(f\"\\nðŸ¤– AI Enhancement Summary:\")\n            print(f\"   â€¢ {additions_count} new metrics/dimensions added\")\n            print(f\"   â€¢ All additions marked with '--- added with AI enhancement'\")\n            print(f\"   â€¢ Ready for immediate use in analytics!\")\n        \n    except Exception as e:\n        print(f\"âŒ Error deploying semantic view: {e}\")\n        print(f\"\\nðŸ” Troubleshooting:\")\n        print(f\"   1. Check if you have CREATE SEMANTIC VIEW privileges\")\n        print(f\"   2. Verify the DDL syntax above is correct\")\n        print(f\"   3. Ensure all referenced tables exist\")\n        print(f\"   4. Try running the DDL manually if needed\")\n        \nelse:\n    print(\"âš ï¸  No enhanced DDL available for deployment\")\n    print(\"   Please run Step 5 first to generate the enhanced DDL\")\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"ðŸ SEMANTIC VIEW ENHANCEMENT WORKFLOW COMPLETE!\")\nprint(\"=\"*60)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75654071-2397-4312-bf5b-7d6abf1277d2",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# Interactive Semantic View Visualization - Streamlit App for Snowflake Notebooks\n# Uses SHOW METRICS and SHOW DIMENSIONS to dynamically discover available metrics and dimensions\n# \n# Usage in Snowflake Notebook:\n# 1. Make sure you have created the HR_SEMANTIC_VIEW\n# 2. Paste this code into a Streamlit cell\n# 3. The app will automatically discover metrics and dimensions\n\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\n\n# Semantic view configuration - adjust if needed\nSEMANTIC_VIEW_NAME = \"HR_SEMANTIC_VIEW\"\nSEMANTIC_VIEW_SCHEMA = \"SV_VHOL_DB.VHOL_SCHEMA\"  # Full schema path\nSEMANTIC_VIEW_FULL_NAME = f\"{SEMANTIC_VIEW_SCHEMA}.{SEMANTIC_VIEW_NAME}\"\n\ndef main():\n    st.title(\"ðŸŽ¯ Semantic View Interactive Visualization\")\n    st.markdown(f\"**Semantic View:** `{SEMANTIC_VIEW_FULL_NAME}`\")\n    \n    # Check if session is available (Snowflake notebook context)\n    if 'session' not in globals():\n        st.error(\"âŒ Snowflake session not available. Please run this in a Snowflake notebook.\")\n        st.info(\"ðŸ’¡ Make sure you're running this in a Snowflake notebook with `session` available\")\n        return\n    \n    # Extract available metrics and dimensions using SHOW commands\n    @st.cache_data\n    def get_options():\n        \"\"\"Get metrics and dimensions from semantic view using SHOW SEMANTIC METRICS/DIMENSIONS commands\n        Returns: (metrics_list, dimensions_list, metrics_map, dimensions_map)\n        where maps contain full_name -> short_name mappings\n        \"\"\"\n        metrics = []\n        dimensions = []\n        metrics_map = {}  # full_name -> short_name\n        dimensions_map = {}  # full_name -> short_name\n        \n        try:\n            # Get metrics from semantic view\n            show_metrics_sql = f\"SHOW SEMANTIC METRICS IN {SEMANTIC_VIEW_FULL_NAME}\"\n            \n            with st.spinner(\"ðŸ” Fetching metrics from semantic view...\"):\n                metrics_result = session.sql(show_metrics_sql).collect()\n            \n            if metrics_result and len(metrics_result) > 0:\n                # Convert to DataFrame to inspect structure\n                metrics_df = pd.DataFrame([dict(row.asDict()) for row in metrics_result])\n                \n                # Debug: Show available columns (first time only)\n                if 'metrics_debug' not in st.session_state:\n                    with st.expander(\"ðŸ” Metrics Result Structure (Debug)\", expanded=False):\n                        st.dataframe(metrics_df.head())\n                        st.write(f\"Columns: {list(metrics_df.columns)}\")\n                    st.session_state.metrics_debug = True\n                \n                # Extract metric names - try common column names\n                metric_name_col = None\n                table_name_col = None\n                \n                for col in ['name', 'metric_name', 'metric', 'METRIC_NAME', 'NAME']:\n                    if col in metrics_df.columns:\n                        metric_name_col = col\n                        break\n                \n                # Try to find table name column\n                for col in ['table_name', 'table', 'TABLE_NAME', 'TABLE', 'source_table', 'entity_name']:\n                    if col in metrics_df.columns:\n                        table_name_col = col\n                        break\n                \n                if metric_name_col:\n                    for _, row in metrics_df.iterrows():\n                        metric_name = str(row[metric_name_col]).strip()\n                        if pd.isna(metric_name) or not metric_name:\n                            continue\n                        \n                        # Try to get table name\n                        table_name = None\n                        if table_name_col and table_name_col in row:\n                            table_name = str(row[table_name_col]).strip()\n                            if pd.isna(table_name) or not table_name:\n                                table_name = None\n                        \n                        # Check if metric_name already contains table prefix (table.metric format)\n                        if '.' in metric_name:\n                            # Already has table prefix\n                            full_name = metric_name\n                            short_name = metric_name.split('.')[-1]\n                            metrics.append(full_name)\n                            metrics_map[full_name] = short_name\n                        elif table_name:\n                            # Create full name with table prefix\n                            full_name = f\"{table_name}.{metric_name}\"\n                            metrics.append(full_name)\n                            metrics_map[full_name] = metric_name\n                        else:\n                            # If no table name, use just the metric name\n                            metrics.append(metric_name)\n                            metrics_map[metric_name] = metric_name\n                else:\n                    # Fallback: use first column\n                    metrics_raw = metrics_df.iloc[:, 0].dropna().unique().tolist()\n                    for metric in metrics_raw:\n                        metrics.append(str(metric))\n                        metrics_map[str(metric)] = str(metric)\n            else:\n                st.warning(\"âš ï¸ No metrics found in semantic view\")\n            \n            # Get dimensions from semantic view\n            show_dimensions_sql = f\"SHOW SEMANTIC DIMENSIONS IN {SEMANTIC_VIEW_FULL_NAME}\"\n            \n            with st.spinner(\"ðŸ” Fetching dimensions from semantic view...\"):\n                dimensions_result = session.sql(show_dimensions_sql).collect()\n            \n            if dimensions_result and len(dimensions_result) > 0:\n                # Convert to DataFrame to inspect structure\n                dimensions_df = pd.DataFrame([dict(row.asDict()) for row in dimensions_result])\n                \n                # Debug: Show available columns (first time only)\n                if 'dimensions_debug' not in st.session_state:\n                    with st.expander(\"ðŸ” Dimensions Result Structure (Debug)\", expanded=False):\n                        st.dataframe(dimensions_df.head())\n                        st.write(f\"Columns: {list(dimensions_df.columns)}\")\n                    st.session_state.dimensions_debug = True\n                \n                # Extract dimension names - try common column names\n                dimension_name_col = None\n                table_name_col = None\n                \n                for col in ['name', 'dimension_name', 'dimension', 'DIMENSION_NAME', 'NAME']:\n                    if col in dimensions_df.columns:\n                        dimension_name_col = col\n                        break\n                \n                # Try to find table name column\n                for col in ['table_name', 'table', 'TABLE_NAME', 'TABLE', 'source_table', 'entity_name']:\n                    if col in dimensions_df.columns:\n                        table_name_col = col\n                        break\n                \n                if dimension_name_col:\n                    for _, row in dimensions_df.iterrows():\n                        dimension_name = str(row[dimension_name_col]).strip()\n                        if pd.isna(dimension_name) or not dimension_name:\n                            continue\n                        \n                        # Try to get table name\n                        table_name = None\n                        if table_name_col and table_name_col in row:\n                            table_name = str(row[table_name_col]).strip()\n                            if pd.isna(table_name) or not table_name:\n                                table_name = None\n                        \n                        # Check if dimension_name already contains table prefix (table.dimension format)\n                        if '.' in dimension_name:\n                            # Already has table prefix\n                            full_name = dimension_name\n                            short_name = dimension_name.split('.')[-1]\n                            dimensions.append(full_name)\n                            dimensions_map[full_name] = short_name\n                        elif table_name:\n                            # Create full name with table prefix\n                            full_name = f\"{table_name}.{dimension_name}\"\n                            dimensions.append(full_name)\n                            dimensions_map[full_name] = dimension_name\n                        else:\n                            # If no table name, use just the dimension name\n                            dimensions.append(dimension_name)\n                            dimensions_map[dimension_name] = dimension_name\n                else:\n                    # Fallback: use first column\n                    dimensions_raw = dimensions_df.iloc[:, 0].dropna().unique().tolist()\n                    for dim in dimensions_raw:\n                        dimensions.append(str(dim))\n                        dimensions_map[str(dim)] = str(dim)\n            else:\n                st.warning(\"âš ï¸ No dimensions found in semantic view\")\n            \n            # Fallback values if nothing found\n            if not metrics and not dimensions:\n                st.error(\"âŒ Could not retrieve metrics or dimensions. Using fallback values.\")\n                st.info(\"ðŸ’¡ Make sure the semantic view exists and is accessible\")\n                metrics = [\"HR_EMPLOYEE_FACT.TOTAL_EMPLOYEES\", \"HR_EMPLOYEE_FACT.AVG_SALARY\", \n                          \"HR_EMPLOYEE_FACT.TOTAL_SALARY_COST\", \"HR_EMPLOYEE_FACT.ATTRITION_COUNT\"]\n                dimensions = [\"DEPARTMENT_DIM.DEPARTMENT_NAME\", \"JOB_DIM.JOB_TITLE\", \n                            \"LOCATION_DIM.LOCATION_NAME\", \"EMPLOYEE_DIM.EMPLOYEE_NAME\"]\n                # Create mappings for fallback\n                for m in metrics:\n                    metrics_map[m] = m.split('.')[-1] if '.' in m else m\n                for d in dimensions:\n                    dimensions_map[d] = d.split('.')[-1] if '.' in d else d\n            elif not metrics:\n                st.warning(\"âš ï¸ No metrics found, using fallback\")\n                metrics = [\"HR_EMPLOYEE_FACT.TOTAL_EMPLOYEES\", \"HR_EMPLOYEE_FACT.AVG_SALARY\", \n                          \"HR_EMPLOYEE_FACT.TOTAL_SALARY_COST\"]\n                for m in metrics:\n                    metrics_map[m] = m.split('.')[-1] if '.' in m else m\n            elif not dimensions:\n                st.warning(\"âš ï¸ No dimensions found, using fallback\")\n                dimensions = [\"DEPARTMENT_DIM.DEPARTMENT_NAME\", \"JOB_DIM.JOB_TITLE\", \n                            \"LOCATION_DIM.LOCATION_NAME\"]\n                for d in dimensions:\n                    dimensions_map[d] = d.split('.')[-1] if '.' in d else d\n            \n        except Exception as e:\n            st.error(f\"âŒ Error fetching metrics/dimensions: {str(e)}\")\n            st.info(\"ðŸ’¡ Using fallback values. Make sure the semantic view exists and is accessible.\")\n            # Fallback values\n            metrics = [\"HR_EMPLOYEE_FACT.TOTAL_EMPLOYEES\", \"HR_EMPLOYEE_FACT.AVG_SALARY\", \n                      \"HR_EMPLOYEE_FACT.TOTAL_SALARY_COST\", \"HR_EMPLOYEE_FACT.ATTRITION_COUNT\"]\n            dimensions = [\"DEPARTMENT_DIM.DEPARTMENT_NAME\", \"JOB_DIM.JOB_TITLE\", \n                        \"LOCATION_DIM.LOCATION_NAME\", \"EMPLOYEE_DIM.EMPLOYEE_NAME\"]\n            # Create mappings for fallback\n            for m in metrics:\n                metrics_map[m] = m.split('.')[-1] if '.' in m else m\n            for d in dimensions:\n                dimensions_map[d] = d.split('.')[-1] if '.' in d else d\n            import traceback\n            with st.expander(\"ðŸ” Error Details\"):\n                st.code(traceback.format_exc(), language='python')\n        \n        # Remove duplicates while preserving order\n        metrics = list(dict.fromkeys(metrics))\n        dimensions = list(dict.fromkeys(dimensions))\n        \n        return metrics, dimensions, metrics_map, dimensions_map\n\n    try:\n        metrics, dimensions, metrics_map, dimensions_map = get_options()\n        \n        if not metrics or not dimensions:\n            st.error(\"âŒ Could not load metrics or dimensions. Please check the semantic view.\")\n            return\n        \n        # Create two columns for the dropdowns\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            selected_metric_full = st.selectbox(\n                \"ðŸ“Š Select Metric:\",\n                metrics,\n                help=\"Choose a metric to visualize\",\n                index=0 if metrics else None\n            )\n        \n        with col2:\n            selected_dimension_full = st.selectbox(\n                \"ðŸ“ Select Dimension:\",\n                dimensions,\n                help=\"Choose a dimension to group by\",\n                index=0 if dimensions else None\n            )\n        \n        if selected_metric_full and selected_dimension_full:\n            # Get short names for ORDER BY (without table prefix)\n            selected_metric_short = metrics_map.get(selected_metric_full, selected_metric_full.split('.')[-1] if '.' in selected_metric_full else selected_metric_full)\n            selected_dimension_short = dimensions_map.get(selected_dimension_full, selected_dimension_full.split('.')[-1] if '.' in selected_dimension_full else selected_dimension_full)\n            \n            # Configuration section\n            st.markdown(\"---\")\n            st.subheader(\"âš™ï¸ Visualization Configuration\")\n            \n            col_config1, col_config2, col_config3, col_config4 = st.columns(4)\n            \n            with col_config1:\n                limit_rows = st.number_input(\n                    \"ðŸ“Š Number of Rows:\",\n                    min_value=1,\n                    max_value=1000,\n                    value=10,\n                    step=1,\n                    help=\"Limit the number of rows returned\"\n                )\n            \n            with col_config2:\n                viz_type = st.selectbox(\n                    \"ðŸ“ˆ Visualization Type:\",\n                    [\"Table\", \"Vertical Bar\", \"Horizontal Bar\", \"Line\", \"Pie\"],\n                    index=1,  # Default to Vertical Bar\n                    help=\"Choose the chart type\"\n                )\n            \n            with col_config3:\n                sort_by = st.selectbox(\n                    \"ðŸ”€ Sort By:\",\n                    [\"Metric\", \"Dimension\"],\n                    index=0,  # Default to Metric\n                    help=\"Choose which column to sort by\"\n                )\n            \n            with col_config4:\n                sort_direction = st.selectbox(\n                    \"â¬†ï¸ Sort Direction:\",\n                    [\"DESC\", \"ASC\"],\n                    index=0,  # Default to DESC\n                    help=\"Choose sort direction\"\n                )\n            \n            # Determine sort column\n            if sort_by == \"Metric\":\n                sort_column = selected_metric_short\n            else:\n                sort_column = selected_dimension_short\n            \n            # Generate semantic SQL using SEMANTIC_VIEW() function\n            # Use full names (with table prefix) inside SEMANTIC_VIEW()\n            # Use short names (without prefix) in ORDER BY outside SEMANTIC_VIEW()\n            query_sql = f\"\"\"SELECT * FROM SEMANTIC_VIEW(\n    {SEMANTIC_VIEW_FULL_NAME}\n    DIMENSIONS {selected_dimension_full}\n    METRICS {selected_metric_full}\n) ORDER BY {sort_column} {sort_direction} LIMIT {limit_rows}\"\"\"\n            \n            # Show the generated SQL in an expander\n            with st.expander(\"ðŸ“‹ View Generated Semantic SQL\"):\n                st.code(query_sql, language='sql')\n            \n            # Execute the query and create visualization\n            try:\n                with st.spinner(\"ðŸ”„ Executing query and creating visualization...\"):\n                    try:\n                        result = session.sql(query_sql).collect()\n                    except Exception as sql_error:\n                        # If full name doesn't work, try with just the view name\n                        if \"SEMANTIC_VIEW\" in str(sql_error).upper() or \"syntax\" in str(sql_error).lower():\n                            st.info(\"ðŸ’¡ Trying with view name only (without schema qualification)...\")\n                            fallback_query = f\"\"\"SELECT * FROM SEMANTIC_VIEW(\n    {SEMANTIC_VIEW_NAME}\n    DIMENSIONS {selected_dimension_full}\n    METRICS {selected_metric_full}\n) ORDER BY {sort_column} {sort_direction} LIMIT {limit_rows}\"\"\"\n                            result = session.sql(fallback_query).collect()\n                            query_sql = fallback_query  # Update the query shown\n                        else:\n                            raise sql_error\n                \n                if result and len(result) > 0:\n                    # Convert to DataFrame\n                    df = pd.DataFrame([dict(row.asDict()) for row in result])\n                    \n                    # Clean column names\n                    df.columns = [col.strip() for col in df.columns]\n                    \n                    # Ensure we have numeric data for the metric\n                    if len(df.columns) >= 2:\n                        # Try to convert metric column to numeric\n                        metric_col = df.columns[1]\n                        df[metric_col] = pd.to_numeric(df[metric_col], errors='coerce')\n                    \n                    # Determine which columns to use\n                    x_col = df.columns[0]\n                    y_col = df.columns[1] if len(df.columns) > 1 else selected_metric_short\n                    \n                    # Explicitly sort the dataframe to maintain SQL sort order\n                    # This ensures Plotly respects the sort order\n                    sort_col_in_df = None\n                    if sort_by == \"Metric\":\n                        sort_col_in_df = y_col\n                    else:\n                        sort_col_in_df = x_col\n                    \n                    # Sort dataframe to match SQL ORDER BY\n                    ascending = (sort_direction == \"ASC\")\n                    df = df.sort_values(by=sort_col_in_df, ascending=ascending).reset_index(drop=True)\n                    \n                    metric_name = selected_metric_short.replace('_', ' ').title()\n                    dimension_name = selected_dimension_short.replace('_', ' ').title()\n                    \n                    # Create visualization based on selected type\n                    if viz_type == \"Table\":\n                        # Show table directly\n                        st.dataframe(df, use_container_width=True)\n                    else:\n                        # Create chart based on type\n                        if viz_type == \"Vertical Bar\":\n                            # Create category order to preserve dataframe sort order\n                            category_order = df[x_col].tolist()\n                            fig = px.bar(\n                                df, \n                                x=x_col, \n                                y=y_col,\n                                title=f'{metric_name} by {dimension_name}',\n                                labels={\n                                    x_col: dimension_name,\n                                    y_col: metric_name\n                                },\n                                color=y_col,\n                                color_continuous_scale='Blues',\n                                category_orders={x_col: category_order}\n                            )\n                            fig.update_layout(\n                                showlegend=False,\n                                height=500,\n                                xaxis_tickangle=-45,\n                                hovermode='x unified',\n                                xaxis={'categoryorder': 'array', 'categoryarray': category_order}\n                            )\n                        \n                        elif viz_type == \"Horizontal Bar\":\n                            # For horizontal bars, preserve y-axis (category) order\n                            category_order = df[x_col].tolist()\n                            fig = px.bar(\n                                df, \n                                x=y_col,\n                                y=x_col,\n                                orientation='h',\n                                title=f'{metric_name} by {dimension_name}',\n                                labels={\n                                    x_col: dimension_name,\n                                    y_col: metric_name\n                                },\n                                color=y_col,\n                                color_continuous_scale='Blues',\n                                category_orders={x_col: category_order}\n                            )\n                            fig.update_layout(\n                                showlegend=False,\n                                height=max(400, len(df) * 30),  # Dynamic height based on rows\n                                hovermode='y unified',\n                                yaxis={'categoryorder': 'array', 'categoryarray': category_order}\n                            )\n                        \n                        elif viz_type == \"Line\":\n                            # Preserve x-axis order for line charts\n                            category_order = df[x_col].tolist()\n                            fig = px.line(\n                                df, \n                                x=x_col, \n                                y=y_col,\n                                title=f'{metric_name} by {dimension_name}',\n                                labels={\n                                    x_col: dimension_name,\n                                    y_col: metric_name\n                                },\n                                markers=True,\n                                category_orders={x_col: category_order}\n                            )\n                            fig.update_layout(\n                                height=500,\n                                xaxis_tickangle=-45,\n                                hovermode='x unified',\n                                xaxis={'categoryorder': 'array', 'categoryarray': category_order}\n                            )\n                        \n                        elif viz_type == \"Pie\":\n                            fig = px.pie(\n                                df,\n                                values=y_col,\n                                names=x_col,\n                                title=f'{metric_name} by {dimension_name}'\n                            )\n                            fig.update_layout(\n                                height=500,\n                                showlegend=True\n                            )\n                            fig.update_traces(textposition='inside', textinfo='percent+label')\n                        \n                        st.plotly_chart(fig, use_container_width=True)\n                    \n                    # Show data table in expander (always available)\n                    with st.expander(\"ðŸ“Š View Data Table\"):\n                        st.dataframe(df, use_container_width=True)\n                    \n                    # Show query execution info\n                    with st.expander(\"ðŸ” Query Execution Details\"):\n                        st.code(query_sql, language='sql')\n                        st.write(f\"**Rows returned:** {len(df)}\")\n                        st.write(f\"**Columns:** {', '.join(df.columns)}\")\n                        if len(df.columns) >= 2:\n                            st.write(f\"**Metric range:** {df[y_col].min():,.2f} to {df[y_col].max():,.2f}\")\n                    \n                    st.success(f\"âœ… Successfully visualized {len(df)} data points!\")\n                    \n                else:\n                    st.warning(\"âš ï¸ No data returned from the semantic view query\")\n                    st.info(\"ðŸ’¡ Try selecting different metrics or dimensions\")\n                    \n            except Exception as e:\n                st.error(f\"âŒ Error executing query: {str(e)}\")\n                st.info(\"ðŸ’¡ Troubleshooting tips:\")\n                st.info(\"1. Make sure the semantic view exists and is accessible\")\n                st.info(\"2. Verify you have proper permissions to query the semantic view\")\n                st.info(\"3. Check that the metric and dimension names are correct\")\n                st.info(\"4. Try the SQL query manually in a SQL cell to debug\")\n                import traceback\n                with st.expander(\"ðŸ” Error Details\"):\n                    st.code(traceback.format_exc(), language='python')\n    \n    except Exception as e:\n        st.error(f\"âŒ Error loading options: {str(e)}\")\n        st.info(\"ðŸ’¡ Make sure the semantic view was created successfully\")\n        import traceback\n        with st.expander(\"ðŸ” Error Details\"):\n            st.code(traceback.format_exc(), language='python')\n\n# Run the Streamlit app\nif __name__ == \"__main__\":\n    main()\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40d0e8b9-20ea-4f6b-9070-47aa7335608d",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "# Natural Language Query Interface for Semantic Views\n# Streamlit App for Snowflake Notebooks\n# Uses Cortex Analyst REST API\n# \n# Usage in Snowflake Notebook:\n# 1. Make sure you're in a Snowflake notebook (not local Streamlit)\n# 2. The 'session' variable should be automatically available\n# 3. Paste this code into a Streamlit cell\n# 4. Select a semantic view from the dropdown\n# 5. Type your natural language question\n# 6. Click \"Answer!\" to execute\n#\n# Note: If session is not available, ensure you're running in a Snowflake notebook environment.\n# The session variable is created automatically when you run a SQL cell in a Snowflake notebook.\n\nimport streamlit as st\nimport pandas as pd\nimport json\nimport time\n\n# Try to import _snowflake (available in Snowflake notebooks)\ntry:\n    import _snowflake  # For interacting with Snowflake-specific APIs\n    SNOWFLAKE_API_AVAILABLE = True\nexcept ImportError:\n    SNOWFLAKE_API_AVAILABLE = False\n    _snowflake = None\n\n# Schema configuration - adjust if needed\nDEFAULT_SCHEMA = \"SV_VHOL_DB.VHOL_SCHEMA\"\n\ndef make_authenticated_request_via_session(session, url, method=\"POST\", json_data=None, headers=None):\n    \"\"\"\n    Attempt to make an HTTP request using the session's connection\n    This bypasses the need for explicit OAuth token extraction\n    \"\"\"\n    try:\n        # Try to get the connection object\n        conn = None\n        if hasattr(session, '_conn'):\n            conn = session._conn\n        elif hasattr(session, 'connection'):\n            conn = session.connection\n        \n        if not conn:\n            return None\n        \n        # Try different methods to make HTTP requests through the connection\n        # Method 1: Check if connection has an HTTP client or request method\n        if hasattr(conn, '_request') or hasattr(conn, 'request'):\n            request_method = getattr(conn, '_request', None) or getattr(conn, 'request', None)\n            if request_method:\n                try:\n                    # Try to make the request\n                    response = request_method(url, method=method, json=json_data, headers=headers)\n                    return response\n                except:\n                    pass\n        \n        # Method 2: Check if there's an HTTP client or session object\n        if hasattr(conn, '_http') or hasattr(conn, 'http') or hasattr(conn, '_session') or hasattr(conn, 'session'):\n            http_client = (getattr(conn, '_http', None) or \n                          getattr(conn, 'http', None) or\n                          getattr(conn, '_session', None) or\n                          getattr(conn, 'session', None))\n            if http_client:\n                try:\n                    if method == \"POST\":\n                        response = http_client.post(url, json=json_data, headers=headers)\n                    else:\n                        response = http_client.request(method, url, json=json_data, headers=headers)\n                    return response\n                except:\n                    pass\n        \n    except Exception:\n        pass\n    \n    return None\n\ndef generate_oauth_token_from_session(session, account, region):\n    \"\"\"\n    Attempt to generate an OAuth token using the current session\n    This uses Snowflake's OAuth API to create a token for REST API calls\n    \"\"\"\n    try:\n        # Try to use Snowflake's OAuth token generation\n        # Note: SYSTEM$GENERATE_OAUTH_TOKEN might not be available\n        try:\n            token_result = session.sql(\"SELECT SYSTEM$GENERATE_OAUTH_TOKEN() as token\").collect()\n            if token_result and len(token_result) > 0:\n                token = token_result[0].get('TOKEN')\n                if token:\n                    return token\n        except:\n            # SYSTEM$GENERATE_OAUTH_TOKEN might not be available\n            pass\n        \n    except Exception as e:\n        # Silently fail\n        pass\n    \n    return None\n\ndef get_auth_token(session):\n    \"\"\"Try to extract authentication token from Snowflake session\"\"\"\n    auth_token = None\n    \n    def _check_object_for_token(obj, depth=0, max_depth=3):\n        \"\"\"Recursively search an object for token-like values\"\"\"\n        if depth > max_depth or obj is None:\n            return None\n        \n        # Check direct token attributes\n        token_attrs = ['_token', 'token', '_master_token', 'master_token', '_session_token', \n                      'session_token', 'access_token', '_access_token', 'bearer_token', '_bearer_token']\n        for attr in token_attrs:\n            if hasattr(obj, attr):\n                try:\n                    value = getattr(obj, attr)\n                    if value and isinstance(value, str) and len(value) > 20:  # Tokens are usually long strings\n                        return value\n                except:\n                    pass\n        \n        # Check if it's a dict-like object\n        if hasattr(obj, '__dict__'):\n            for key, value in obj.__dict__.items():\n                if 'token' in key.lower() and isinstance(value, str) and len(value) > 20:\n                    return value\n                # Recursively check nested objects (but limit depth)\n                if depth < max_depth and isinstance(value, object) and not isinstance(value, (str, int, float, bool)):\n                    result = _check_object_for_token(value, depth + 1, max_depth)\n                    if result:\n                        return result\n        \n        return None\n    \n    try:\n        # Try to get from session's connection\n        conn = None\n        \n        # Method 1: Try session._conn (Snowpark)\n        if hasattr(session, '_conn'):\n            conn = session._conn\n        # Method 2: Try session.connection (alternative attribute name)\n        elif hasattr(session, 'connection'):\n            conn = session.connection\n        # Method 3: Try session._connection (another variant)\n        elif hasattr(session, '_connection'):\n            conn = session._connection\n        \n        if conn:\n            # Method A: Try REST client token (for Python connector connections)\n            if hasattr(conn, '_rest'):\n                rest_client = conn._rest\n                # Try direct attributes first\n                for token_attr in ['_token', 'token', '_master_token', 'master_token', '_session_token']:\n                    if hasattr(rest_client, token_attr):\n                        try:\n                            token_value = getattr(rest_client, token_attr)\n                            if token_value and isinstance(token_value, str) and len(token_value) > 20:\n                                auth_token = token_value\n                                break\n                        except:\n                            pass\n                \n                # Try recursive search if direct access failed\n                if not auth_token:\n                    auth_token = _check_object_for_token(rest_client, max_depth=2)\n                \n                # Try token manager if available\n                if not auth_token and hasattr(rest_client, '_token_manager'):\n                    token_manager = rest_client._token_manager\n                    auth_token = _check_object_for_token(token_manager, max_depth=2)\n            \n            # Method A2: For ServerConnection (Snowflake notebooks), try different attributes\n            # ServerConnection might have token stored differently\n            if not auth_token:\n                # Try connection-level token attributes\n                auth_token = _check_object_for_token(conn, max_depth=3)\n            \n            # Method A3: Try to get from connection's internal state\n            if not auth_token:\n                # Check for session token or authentication state\n                internal_attrs = ['_session_token', '_auth_token', '_token', 'token', \n                                 '_session', '_authenticator', '_login_manager']\n                for attr in internal_attrs:\n                    if hasattr(conn, attr):\n                        try:\n                            value = getattr(conn, attr)\n                            if isinstance(value, str) and len(value) > 20:\n                                auth_token = value\n                                break\n                            elif hasattr(value, '__dict__'):\n                                # If it's an object, search it recursively\n                                token = _check_object_for_token(value, max_depth=2)\n                                if token:\n                                    auth_token = token\n                                    break\n                        except:\n                            pass\n            \n            # Method B: Try connection-level token attributes (recursive)\n            if not auth_token:\n                auth_token = _check_object_for_token(conn, max_depth=3)\n            \n            # Method C: Try from connection's authentication handler\n            if not auth_token:\n                auth_attrs = ['_authenticate', '_auth', 'authenticate', '_auth_handler', 'auth_handler']\n                for auth_attr in auth_attrs:\n                    if hasattr(conn, auth_attr):\n                        try:\n                            auth_handler = getattr(conn, auth_attr)\n                            auth_token = _check_object_for_token(auth_handler, max_depth=2)\n                            if auth_token:\n                                break\n                        except:\n                            pass\n            \n            # Method D: Try to get from connection's headers/cookies\n            if not auth_token and hasattr(conn, '_rest'):\n                rest_client = conn._rest\n                # Check if there's a headers dict with authorization\n                header_attrs = ['_headers', 'headers', '_request_headers', 'request_headers']\n                for header_attr in header_attrs:\n                    if hasattr(rest_client, header_attr):\n                        try:\n                            headers = getattr(rest_client, header_attr)\n                            if isinstance(headers, dict):\n                                auth_header = headers.get('Authorization') or headers.get('authorization')\n                                if auth_header and isinstance(auth_header, str):\n                                    if auth_header.startswith('Bearer '):\n                                        auth_token = auth_header[7:]  # Remove 'Bearer ' prefix\n                                    else:\n                                        auth_token = auth_header\n                                    if auth_token:\n                                        break\n                        except:\n                            pass\n    \n    except Exception as e:\n        # Silently fail - we'll handle missing token in the UI\n        pass\n    \n    return auth_token\n\ndef main():\n    st.title(\"ðŸ’¬ Natural Language Query for Semantic Views\")\n    st.markdown(\"Ask questions in plain English about your semantic view data\")\n    st.markdown(\"*Using [Cortex Analyst REST API](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst/rest-api)*\")\n    \n    # Check if session is available (Snowflake notebook context)\n    # In Snowflake notebooks, session is typically available as a global variable\n    if 'session' not in globals():\n        st.error(\"âŒ Snowflake session not available. Please run this in a Snowflake notebook.\")\n        st.info(\"ðŸ’¡ Make sure you're running this in a Snowflake notebook with `session` available\")\n        return\n    \n    # Get account and region info early - cache it for the session\n    @st.cache_data\n    def get_account_info():\n        \"\"\"Get account and region from the current Snowflake session\"\"\"\n        try:\n            account_info = session.sql(\"SELECT CURRENT_ACCOUNT() as account, CURRENT_REGION() as region\").collect()\n            if account_info and len(account_info) > 0:\n                account = account_info[0]['ACCOUNT']\n                region = account_info[0]['REGION']\n                return account, region\n        except Exception:\n            pass\n        return None, None\n    \n    # Pre-populate account and region first (needed for token generation)\n    account, region = get_account_info()\n    \n    # Get token early - cache it for the session\n    @st.cache_data\n    def get_cached_token(account_val, region_val):\n        \"\"\"Get auth token from session - cached, tries extraction then generation\"\"\"\n        # First try to extract existing token\n        token = get_auth_token(session)\n        \n        # If extraction failed and we have account/region, try generating one\n        if not token and account_val and region_val:\n            try:\n                token = generate_oauth_token_from_session(session, account_val, region_val)\n            except:\n                pass\n        \n        return token\n    \n    # Check if _snowflake API is available (required for authentication)\n    if account and region:\n        if not SNOWFLAKE_API_AVAILABLE:\n            st.error(\"âš ï¸ `_snowflake` module not available. This app requires running in a Snowflake notebook.\")\n            st.info(\"ðŸ’¡ The `_snowflake` module provides automatic authentication for REST API calls.\")\n            return\n    else:\n        st.warning(\"âš ï¸ Could not retrieve account information. Some features may not work.\")\n    \n    # Get available semantic views in the schema\n    @st.cache_data\n    def get_semantic_views(schema_name):\n        \"\"\"Get list of available semantic views in the schema\"\"\"\n        try:\n            # Handle schema name (could be \"DATABASE.SCHEMA\" or just \"SCHEMA\")\n            if '.' in schema_name:\n                database, schema = schema_name.split('.', 1)\n                show_sql = f\"SHOW SEMANTIC VIEWS IN SCHEMA {database}.{schema}\"\n            else:\n                # Try to use current database context\n                show_sql = f\"SHOW SEMANTIC VIEWS IN SCHEMA {schema_name}\"\n            \n            result = session.sql(show_sql).collect()\n            \n            if result and len(result) > 0:\n                # Convert to DataFrame\n                views_df = pd.DataFrame([dict(row.asDict()) for row in result])\n                \n                # Try to find the name column\n                name_col = None\n                for col in ['name', 'semantic_view_name', 'view_name', 'NAME', 'SEMANTIC_VIEW_NAME']:\n                    if col in views_df.columns:\n                        name_col = col\n                        break\n                \n                if name_col:\n                    views = views_df[name_col].dropna().unique().tolist()\n                else:\n                    # Fallback: use first column\n                    views = views_df.iloc[:, 0].dropna().unique().tolist()\n                \n                # Create full qualified names\n                full_names = []\n                for view in views:\n                    full_name = f\"{schema_name}.{view}\" if '.' not in view else view\n                    full_names.append(full_name)\n                \n                return full_names, views_df\n            else:\n                return [], pd.DataFrame()\n                \n        except Exception as e:\n            st.error(f\"âŒ Error fetching semantic views: {str(e)}\")\n            return [], pd.DataFrame()\n    \n    # Schema selection\n    schema_input = st.text_input(\n        \"ðŸ“ Schema:\",\n        value=DEFAULT_SCHEMA,\n        help=\"Enter the schema path (e.g., DATABASE.SCHEMA)\"\n    )\n    \n    # Get semantic views\n    with st.spinner(\"ðŸ” Loading semantic views...\"):\n        semantic_views, views_df = get_semantic_views(schema_input)\n    \n    if not semantic_views:\n        st.warning(f\"âš ï¸ No semantic views found in {schema_input}\")\n        st.info(\"ðŸ’¡ Make sure the schema name is correct and contains semantic views\")\n        \n        # Show debug info if available\n        if not views_df.empty:\n            with st.expander(\"ðŸ” Debug: SHOW SEMANTIC VIEWS Result\"):\n                st.dataframe(views_df)\n        return\n    \n    # Semantic view selection\n    selected_view = st.selectbox(\n        \"ðŸ“Š Select Semantic View:\",\n        semantic_views,\n        help=\"Choose a semantic view to query\",\n        index=0 if semantic_views else None\n    )\n    \n    if selected_view:\n        st.markdown(\"---\")\n        \n        # Natural language question input\n        st.subheader(\"ðŸ’¬ Ask Your Question\")\n        question = st.text_area(\n            \"Enter your question:\",\n            height=100,\n            placeholder=\"e.g., What are the top 5 departments by average salary?\",\n            help=\"Type your question in natural language\"\n        )\n        \n        # Answer button\n        col1, col2 = st.columns([1, 4])\n        with col1:\n            answer_button = st.button(\"ðŸš€ Answer!\", type=\"primary\", use_container_width=True)\n        \n        if answer_button and question:\n            if not question.strip():\n                st.warning(\"âš ï¸ Please enter a question\")\n            else:\n                # Generate SQL from natural language question using Cortex Analyst REST API\n                generated_sql = None  # Initialize outside try block\n                \n                try:\n                    with st.spinner(\"ðŸ¤– Generating SQL from your question...\"):\n                        # Use Snowflake's built-in API request method (no token needed!)\n                        if not SNOWFLAKE_API_AVAILABLE:\n                            st.error(\"âŒ `_snowflake` module not available. Make sure you're running this in a Snowflake notebook.\")\n                            st.info(\"ðŸ’¡ The `_snowflake` module is automatically available in Snowflake notebooks.\")\n                            return\n                        \n                        # Build request body for Cortex Analyst API\n                        # According to Snowflake Labs example: https://github.com/Snowflake-Labs/sfguide-getting-started-with-cortex-analyst\n                        # Note: API requires exactly one of: semantic_model, semantic_model_file, or semantic_view\n                        request_body = {\n                            \"messages\": [\n                                {\n                                    \"role\": \"user\",\n                                    \"content\": [\n                                        {\n                                            \"type\": \"text\",\n                                            \"text\": question\n                                        }\n                                    ]\n                                }\n                            ],\n                            \"semantic_view\": selected_view\n                        }\n                        \n                        # Use Snowflake's built-in API request method\n                        # This automatically handles authentication - no token needed!\n                        API_ENDPOINT = \"/api/v2/cortex/analyst/message\"\n                        API_TIMEOUT = 50000  # in milliseconds\n                        \n                        resp = _snowflake.send_snow_api_request(\n                            \"POST\",  # method\n                            API_ENDPOINT,  # path\n                            {},  # headers (empty - auth is handled automatically)\n                            {},  # params\n                            request_body,  # body\n                            None,  # request_guid\n                            API_TIMEOUT,  # timeout in milliseconds\n                        )\n                        \n                        # Parse response\n                        # Content is a string with serialized JSON object\n                        parsed_content = json.loads(resp[\"content\"])\n                        \n                        # Check if the response is successful\n                        if resp[\"status\"] >= 400:\n                            # Error response\n                            error_msg = f\"\"\"\nðŸš¨ An Analyst API error has occurred ðŸš¨\n\n* response code: `{resp['status']}`\n* request-id: `{parsed_content.get('request_id', 'N/A')}`\n* error code: `{parsed_content.get('error_code', 'N/A')}`\n\nMessage:\n\n{parsed_content.get('message', 'Unknown error')}\n\n                            \"\"\"\n                            st.error(error_msg)\n                            generated_sql = None\n                        else:\n                            # Success - extract response data\n                            response_data = parsed_content\n                            \n                            # Extract SQL from response\n                            # Response structure: message.content[] with type \"sql\" containing \"statement\"\n                            text_response = None\n                            \n                            if 'message' in response_data and 'content' in response_data['message']:\n                                for content_block in response_data['message']['content']:\n                                    if content_block.get('type') == 'sql':\n                                        generated_sql = content_block.get('statement', '')\n                                    elif content_block.get('type') == 'text':\n                                        text_response = content_block.get('text', '')\n                            \n                            # Show text interpretation if available\n                            if text_response:\n                                with st.expander(\"ðŸ“ Interpretation\", expanded=False):\n                                    st.write(text_response)\n                            \n                            # Show warnings if any\n                            if 'warnings' in response_data and response_data['warnings']:\n                                for warning in response_data['warnings']:\n                                    st.warning(f\"âš ï¸ {warning.get('message', 'Warning')}\")\n                            \n                            if generated_sql:\n                                # Show generated SQL\n                                with st.expander(\"ðŸ” Generated SQL Query\", expanded=False):\n                                    st.code(generated_sql, language='sql')\n                                \n                                # Show response metadata if available\n                                if 'response_metadata' in response_data:\n                                    with st.expander(\"ðŸ“Š Response Metadata\", expanded=False):\n                                        st.json(response_data['response_metadata'])\n                            else:\n                                # Check if suggestions were provided\n                                suggestions_found = False\n                                if 'message' in response_data and 'content' in response_data['message']:\n                                    for content_block in response_data['message']['content']:\n                                        if content_block.get('type') == 'suggestions':\n                                            st.info(\"ðŸ’¡ Your question might be ambiguous. Here are some suggestions:\")\n                                            suggestions = content_block.get('suggestions', [])\n                                            for i, suggestion in enumerate(suggestions, 1):\n                                                st.write(f\"{i}. {suggestion}\")\n                                            suggestions_found = True\n                                \n                                if not suggestions_found:\n                                    st.error(\"âŒ No SQL generated. Check the response for details.\")\n                                    with st.expander(\"ðŸ” Full Response\"):\n                                        st.json(response_data)\n                                    generated_sql = None  # Ensure it's None if no SQL generated\n                        \n                        # Execute the query if SQL was generated\n                        if generated_sql:\n                            with st.spinner(\"ðŸ”„ Executing query...\"):\n                                try:\n                                    result = session.sql(generated_sql).collect()\n                                    \n                                    if result and len(result) > 0:\n                                        # Convert to DataFrame\n                                        df = pd.DataFrame([dict(row.asDict()) for row in result])\n                                        \n                                        # Display results\n                                        st.subheader(\"ðŸ“Š Results\")\n                                        st.dataframe(df, use_container_width=True)\n                                        \n                                        # Show summary\n                                        st.success(f\"âœ… Query executed successfully! Returned {len(df)} rows.\")\n                                        \n                                        # Show query details\n                                        with st.expander(\"ðŸ“‹ Query Details\"):\n                                            st.code(generated_sql, language='sql')\n                                            st.write(f\"**Rows returned:** {len(df)}\")\n                                            st.write(f\"**Columns:** {', '.join(df.columns)}\")\n                                        \n                                    else:\n                                        st.info(\"â„¹ï¸ Query executed but returned no results.\")\n                                        \n                                except Exception as e:\n                                    st.error(f\"âŒ Error executing query: {str(e)}\")\n                                    st.info(\"ðŸ’¡ The generated SQL might need adjustment. Check the generated SQL above.\")\n                                    import traceback\n                                    with st.expander(\"ðŸ” Error Details\"):\n                                        st.code(traceback.format_exc(), language='python')\n                        \n                        else:\n                            st.error(\"âŒ Could not generate SQL from Cortex Analyst API\")\n                            st.info(\"ðŸ’¡ Check the API response above for details.\")\n                    \n                except Exception as e:\n                    st.error(f\"âŒ Error generating SQL: {str(e)}\")\n                    st.info(\"ðŸ’¡ Make sure you're running in a Snowflake notebook and that Cortex Analyst is available in your account.\")\n                    import traceback\n                    with st.expander(\"ðŸ” Error Details\"):\n                        st.code(traceback.format_exc(), language='python')\n    \n    # Show available semantic views info\n    with st.expander(\"â„¹ï¸ About This App\"):\n        st.markdown(\"\"\"\n        **How to use:**\n        1. Select a semantic view from the dropdown\n        2. Type your question in natural language\n        3. Click \"Answer!\" to generate and execute the query\n        \n        **Example questions:**\n        - \"What are the top 10 departments by total employees?\"\n        - \"Show me average salary by job title\"\n        - \"Which locations have the highest attrition rates?\"\n        - \"List the top 5 employees by salary\"\n        \n        **Note:** This app uses the [Cortex Analyst REST API](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst/rest-api) \n        to generate SQL from natural language questions. The API automatically understands your semantic view \n        structure and generates appropriate queries.\n        \n        **Authentication:** The app attempts to automatically retrieve your authentication token from the session.\n        If that fails, you can manually enter an OAuth token when prompted.\n        \"\"\")\n\n# Run the Streamlit app\nif __name__ == \"__main__\":\n    main()\n\n\n",
   "execution_count": null
  }
 ]
}