{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "4ko35pvc6474aqxt4s6u",
   "authorId": "2398867098351",
   "authorName": "FUKAMORI",
   "authorEmail": "kenshiro.fukamori@snowflake.com",
   "sessionId": "e4815905-b2e3-4c59-b810-2cd2895b76ba",
   "lastEditTime": 1765933212566
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c24f87-a280-473e-8b94-dc38058a0e20",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "dd9a4e0a-db96-49c4-a452-cab93f499f36",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "# CortexAI„ÇíÁî®„ÅÑ„ÅüÊã°Âºµ„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„ÅÆ‰ΩúÊàê\n### ÈÅéÂéª„ÅÆ„ÇØ„Ç®„É™Â±•Ê≠¥ÔºàSeed QueriesÔºâ„Åã„Çâ„ÄÅ„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„Çí‰ΩúÊàê„Åô„Çã\n### ‰ΩúÊàê„Å´„ÅÇ„Åü„Å£„Å¶„ÅØCortex AI„ÅÆLLMÈñ¢Êï∞„ÅÆCompleteÈñ¢Êï∞„ÇíÂà©Áî®„Åô„Çã"
  },
  {
   "cell_type": "markdown",
   "id": "530bdf82-fdd6-4e81-95ef-9adfdca85818",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "‰∫ãÂâçÊ∫ñÂÇôÔºöPackages„Åã„Çâplotly„Çí„Ç§„É≥„Çπ„Éà„Éº„É´"
  },
  {
   "cell_type": "markdown",
   "id": "499eda06-9c75-41b6-a13a-2cb931313399",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "07ce6bab-3ee2-400f-9ceb-f053413dc7a0",
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": "//Âøµ„ÅÆÁÇ∫„Å´„É™„Ç∂„É´„Éà„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíOFF„Å´„Åó„Å¶„Åä„Åç„Åæ„Åô„ÄÇÔºàÈÄî‰∏≠„Åß„Ç≥„Éº„Éâ„ÅåÂ§±Êïó„Åó„Åü„Å®„Åç„Å´Êñ∞„Åó„ÅÑÂÜÖÂÆπ„ÅßÁµêÊûú„Å®ÂèñÂæó„ÅóÁõ¥„Åô„Åü„ÇÅÔºâ\nalter session set use_cached_result = false;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f5af2a2-b994-46de-8d99-cfb19ed65bd7",
   "metadata": {
    "language": "sql",
    "name": "cell23"
   },
   "outputs": [],
   "source": "ALTER ACCOUNT SET CORTEX_ENABLED_CROSS_REGION = 'ANY_REGION';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d862617f-394f-427e-9ae8-0fa905a1cbee",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "## „Åì„ÅÆ„Çª„É´„Åß„ÅØ„ÄÅAI „ÇíÁî®„ÅÑ„Åü„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„ÉºÊã°Âºµ„ÅÆ„Åü„ÇÅ„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ\n\n- ÂøÖË¶Å„Å™ Python „É©„Ç§„Éñ„É©„É™Ôºàjson, re, pandas, Snowpark „Å™„Å©Ôºâ„Çí„Ç§„É≥„Éù„Éº„Éà\n- `get_active_session()` „Åß Snowflake „ÅÆ Snowpark „Çª„ÉÉ„Ç∑„Éß„É≥„ÇíÂèñÂæó\n- Ëß£ÊûêÂØæË±°ÊôÇÈñì (`HOURS_BACK`)„ÄÅÂØæË±°„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„ÉºÂêç (`SEMANTIC_VIEW_NAME`)„ÄÅ‰ΩøÁî®„Åô„Çã Cortex „É¢„Éá„É´ (`CORTEX_MODEL`) „ÇíÂÆöÁæ©\n- `USE ROLE / USE DATABASE / USE SCHEMA` „Åß„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Çí `agentic_analytics_vhol_role / SV_VHOL_DB / VHOL_SCHEMA` „Å´Âàá„ÇäÊõø„Åà\n- `CURRENT_DATABASE()` „Å™„Å©„ÇíÂèñÂæó„Åó„ÄÅÁèæÂú®„ÅÆ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# setup for AI-powered enrichment\n# Import required libraries (available in Snowflake notebooks)\nimport json\nimport re\nimport pandas as pd\nfrom typing import List, Dict, Any\nfrom snowflake.snowpark import Session\n\n# Get the built-in Snowpark session\nsession = get_active_session()\n\n# Configuration\nHOURS_BACK = 12  # How many hours back to look in query history\nSEMANTIC_VIEW_NAME = 'HR_SEMANTIC_VIEW'\nCORTEX_MODEL = 'claude-4-sonnet'  # Claude model with high token limit\n\n# Set context for the analysis\nsession.sql(\"USE ROLE agentic_analytics_vhol_role\").collect()\nsession.sql(\"USE DATABASE SV_VHOL_DB\").collect()\nsession.sql(\"USE SCHEMA VHOL_SCHEMA\").collect()\n\n# Verify connection\ncurrent_context = session.sql(\"\"\"\n    SELECT \n        CURRENT_DATABASE() as database,\n        CURRENT_SCHEMA() as schema,\n        CURRENT_WAREHOUSE() as warehouse,\n        CURRENT_ROLE() as role,\n        CURRENT_USER() as user\n\"\"\").collect()\n\ncurrent_context",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "23208a6f-3af9-4fcb-aedd-5d619eed26f5",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "### „Åì„ÅÆ„Çª„É´„Åß„ÅØ„ÄÅQUERY_HISTORY„Åã„Çâ VHOL ÊâÄÊúõ„ÅÆ„ÇØ„Ç®„É™ÔºàSeed QueryÔºâ„ÅÆÂ±•Ê≠¥„ÇíÂèñÂæó„Åó„Åæ„Åô„ÄÇ\n\n- „Çª„ÉÉ„Ç∑„Éß„É≥„ÅÆ„Çø„Ç§„É†„Çæ„Éº„É≥„Çí `Asia/Tokyo` „Å´Ë®≠ÂÆö\n- ÈÅéÂéª `HOURS_BACK` ÊôÇÈñì„ÅÆ‰∏≠„Åß„ÄÅ\n  - `QUERY_TEXT ILIKE '%VHOL_Seed_Query%'`\n  - `EXECUTION_STATUS = 'SUCCESS'`\n  „ÇíÊ∫Ä„Åü„Åô„ÇØ„Ç®„É™„ÇíÁµû„ÇäËæº„Åø\n- ÁµêÊûú„Çí pandas „ÅÆ DataFrame (`query_history_df`) „Å´Â§âÊèõ\n- ‰ª∂Êï∞„Å®ÂÖàÈ†≠Êï∞‰ª∂„ÅÆ„ÇØ„Ç®„É™Êú¨Êñá„Çí„É≠„Ç∞Âá∫Âäõ„Åó„ÄÅSeed Query „ÅåÂ≠òÂú®„Åô„Çã„Åã„Å©„ÅÜ„Åã„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ"
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "# Query to retrieve VHOL Seed Queries from history\nquery_alter_timezone = f\"\"\" ALTER SESSION SET TIMEZONE = 'Asia/Tokyo' \"\"\"\n\nquery_history_sql = f\"\"\"\nSELECT \n    QUERY_TEXT,\n    START_TIME,\n    EXECUTION_STATUS,\n    USER_NAME\nFROM \n    SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\nWHERE \n    START_TIME >= DATEADD('hour', -{HOURS_BACK}, CURRENT_TIMESTAMP())\n    AND QUERY_TEXT ILIKE '%VHOL_Seed_Query%'\n    AND QUERY_TEXT NOT ILIKE '%QUERY_TEXT%'\n    AND EXECUTION_STATUS = 'SUCCESS'\nORDER BY \n    START_TIME DESC\nLIMIT 50\n\"\"\"\n\nprint(f\"üîç Retrieving VHOL Seed Queries from last {HOURS_BACK} hours...\")\n\n# Execute query and convert to pandas DataFrame\nquery_history_result = session.sql(query_alter_timezone)\nquery_history_result = session.sql(query_history_sql).collect()\nquery_history_df = pd.DataFrame([dict(row.asDict()) for row in query_history_result])\n\nprint(f\"üìä Found {len(query_history_df)} VHOL Seed Queries in the last {HOURS_BACK} hours\")\n\nif len(query_history_df) > 0:\n    print(\"\\nSample queries found:\")\n    for i, row in query_history_df.head(3).iterrows():\n        print(f\"\\n{i+1}. Query at {row['START_TIME']}:\")\n        # Show first 1000 characters of query\n        query_preview = row['QUERY_TEXT'][:1000] + \"...\" if len(row['QUERY_TEXT']) > 1000 else row['QUERY_TEXT']\n        print(f\"   {query_preview}\")\nelse:\n    print(\"‚ö†Ô∏è  No VHOL Seed Queries found. You may need to:\")\n    print(\"   1. Run some queries with 'VHOL Seed Query' comments\")\n    print(\"   2. Increase the HOURS_BACK parameter\")\n    print(\"   3. Check that the queries executed successfully\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cca48a71-5aa4-40a4-b683-ee8323c39bda",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "//Query_History„ÅåÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç„ÅÆ„Åü„ÇÅ„ÅÆSQL\n//ÂèçÊò†„Å´Â∞ë„ÅóÊôÇÈñì„Åå„Åã„Åã„Çã\n\nALTER SESSION SET TIMEZONE = 'Asia/Tokyo';\n\nSELECT \n    QUERY_TEXT,\n    START_TIME,\n    EXECUTION_STATUS,\n    USER_NAME\nFROM \n    SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\nWHERE \n    START_TIME >= DATEADD('hour', -12, CURRENT_TIMESTAMP())\n    AND QUERY_TEXT ILIKE '%vhol_seed_query%'\n    AND EXECUTION_STATUS = 'SUCCESS'\nORDER BY \n    START_TIME DESC\nLIMIT 50",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "655e8dbe-94e9-4a1c-9a00-55cc44ee77d0",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "### „Åì„ÅÆ„Çª„É´„Åß„ÅØ„ÄÅ„ÇØ„Ç®„É™Â±•Ê≠¥„Å´Âê´„Åæ„Çå„Çã SQL „Åã„Çâ„ÄÅ„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„ÉºÊã°Âºµ„Å´Âà©Áî®„Åß„Åç„Çã„É°„Éà„É™„ÇØ„Çπ„Å®„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥ÂÄôË£ú„ÇíËá™ÂãïÊäΩÂá∫„Åó„Åæ„Åô„ÄÇ\n\n- `extract_metrics_and_dimensions` Èñ¢Êï∞„Åß‰ª•‰∏ã„ÇíÂÆüÊñΩ:\n  - „Ç≥„É°„É≥„Éà„ÇíÂâäÈô§„Åó„Å¶ SQL „ÇíÊ≠£Ë¶èÂåñ\n  - ÈõÜË®àÈñ¢Êï∞ÔºàCOUNT, SUM, AVG, MIN, MAX, STDDEV, PERCENTILE_CONT, ROUND „Å™„Å©Ôºâ„Çí„É°„Éà„É™„ÇØ„ÇπÂÄôË£ú„Å®„Åó„Å¶ÊäΩÂá∫\n  - SELECT / WHERE / GROUP BY / EXTRACT / DATEDIFF ÂÜÖ„ÅÆ `„ÉÜ„Éº„Éñ„É´.„Ç´„É©„É†` ÂèÇÁÖß„Çí„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥ÂÄôË£ú„Å®„Åó„Å¶ÊäΩÂá∫\n- „Åô„Åπ„Å¶„ÅÆ Seed Query „ÇíËß£Êûê„Åó„ÄÅÈáçË§á„ÇíÈô§„ÅÑ„Åü `unique_metrics` / `unique_dimensions` „Çí‰ΩúÊàê\n- VHOL Âõ∫Êúâ„ÅÆ„ÉÜ„Éº„Éñ„É´„Ç®„Ç§„É™„Ç¢„ÇπÔºàF, E, D, J, LÔºâ„ÇíÂÆü„ÉÜ„Éº„Éñ„É´ÂêçÔºàHR_EMPLOYEE_FACT, EMPLOYEE_DIM „Å™„Å©Ôºâ„Å´ÁΩÆ„ÅçÊèõ„Åà\n- ÊäΩÂá∫„Åï„Çå„Åü„É°„Éà„É™„ÇØ„Çπ„Éª„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„ÅÆÊï∞„Å®„Çµ„É≥„Éó„É´„Çí„É≠„Ç∞„Å´Âá∫Âäõ„Åó„Åæ„Åô„ÄÇ"
  },
  {
   "cell_type": "code",
   "id": "eced9238-fe01-4e4b-84e5-c2b3cc303f05",
   "metadata": {
    "language": "sql",
    "name": "cell21"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33d91ee6-0312-41b5-86f2-2fc97c7c58f6",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "def extract_metrics_and_dimensions(query_text: str) -> Dict[str, List[str]]:\n    \"\"\"\n    SQL„ÇØ„Ç®„É™ÊñáÂ≠óÂàó„Åã„Çâ„É°„Éà„É™„ÇØ„ÇπÔºàÈõÜË®àÈñ¢Êï∞Ôºâ„Å®„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥Ôºà„ÉÜ„Éº„Éñ„É´.„Ç´„É©„É†ÂèÇÁÖßÔºâ„ÇíÊäΩÂá∫„Åô„Çã\n    \"\"\"\n    metrics = []\n    dimensions = []\n\n    # ÂÖÉ„ÇØ„Ç®„É™„Åã„Çâ„Ç≥„É°„É≥„Éà„ÇíÂâäÈô§„Åó„Å¶Ê≠£Ë¶èÂåñ\n    # 1Ë°å„Ç≥„É°„É≥„ÉàÔºà-- ...Ôºâ„ÇíÈô§Âéª\n    query_clean = re.sub(r'--.*?\\n', '\\n', query_text)\n    # „Éñ„É≠„ÉÉ„ÇØ„Ç≥„É°„É≥„ÉàÔºà/* ... */Ôºâ„ÇíÈô§Âéª\n    query_clean = re.sub(r'/\\*.*?\\*/', '', query_clean, flags=re.DOTALL)\n    # Â§ßÊñáÂ≠óÂåñ„Åó„Å¶Ê≠£Ë¶èË°®Áèæ„Éû„ÉÉ„ÉÅ„Çí„Åó„ÇÑ„Åô„Åè„Åô„Çã\n    query_upper = query_clean.upper()\n\n    # ÈõÜË®àÈñ¢Êï∞Ôºà„É°„Éà„É™„ÇØ„ÇπÔºâ„ÅÆ„Éë„Çø„Éº„É≥ÂÆöÁæ©\n    metric_patterns = [\n        r'COUNT\\s*\\([^)]+\\)',\n        r'SUM\\s*\\([^)]+\\)',\n        r'AVG\\s*\\([^)]+\\)',\n        r'MIN\\s*\\([^)]+\\)',\n        r'MAX\\s*\\([^)]+\\)',\n        r'STDDEV\\s*\\([^)]+\\)',\n        r'PERCENTILE_CONT\\s*\\([^)]+\\)',\n        r'ROUND\\s*\\([^)]+\\)',\n    ]\n\n    # ‰∏äË®ò„Éë„Çø„Éº„É≥„Å´„Éû„ÉÉ„ÉÅ„Åô„ÇãÊñáÂ≠óÂàó„Çí„Åô„Åπ„Å¶„É°„Éà„É™„ÇØ„ÇπÂÄôË£ú„Å®„Åó„Å¶ÊäΩÂá∫\n    for pattern in metric_patterns:\n        matches = re.findall(pattern, query_upper)\n        metrics.extend(matches)\n\n    # „Ç´„É©„É†ÂèÇÁÖßÔºà„ÉÜ„Éº„Éñ„É´.„Ç´„É©„É†Ôºâ„ÅÆ„Éë„Çø„Éº„É≥ÂÆöÁæ©\n    # SELECT / WHERE / GROUP BY / EXTRACT / DATEDIFF „ÅÆÊñáËÑà„ÅßÂá∫„Å¶„Åè„Çã„ÇÇ„ÅÆ„ÇíÂØæË±°„Å®„Åô„Çã\n    column_patterns = [\n        r'SELECT\\s+.*?([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)',   # SELECTÂè•ÂÜÖ„ÅÆ table.column\n        r'WHERE\\s+.*?([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)',    # WHEREÂè•ÂÜÖ„ÅÆ table.column\n        r'GROUP BY\\s+.*?([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)', # GROUP BYÂè•ÂÜÖ„ÅÆ table.column\n        r'EXTRACT\\s*\\(\\s*[A-Z]+\\s+FROM\\s+([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)\\)',  # EXTRACT(FROM table.column)\n        r'DATEDIFF\\s*\\([^,]+,\\s*([A-Z_][A-Z0-9_]*\\.[A-Z_][A-Z0-9_]*)',             # DATEDIFF(..., table.column)\n    ]\n\n    # ÂêÑ„Éë„Çø„Éº„É≥„Å´„Éû„ÉÉ„ÉÅ„Åó„Åü table.column „Çí„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥ÂÄôË£ú„Å®„Åó„Å¶ÂèéÈõÜ\n    for pattern in column_patterns:\n        matches = re.findall(pattern, query_upper)\n        for match in matches:\n            # ÈõÜË®àÈñ¢Êï∞„ÅÆ‰∏ÄÈÉ®ÔºàCOUNT(x.xxx) „Å™„Å©Ôºâ„Å®„Åó„Å¶Ê§úÂá∫„Åï„Çå„Åü„ÇÇ„ÅÆ„ÅØÈô§Â§ñ„Åó„ÄÅÁ¥îÁ≤ã„Å™„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„Å†„Åë„ÇíÊÆã„Åô\n            if not any(agg in match for agg in ['COUNT', 'SUM', 'AVG', 'MIN', 'MAX']):\n                dimensions.append(match)\n\n    # ‰ΩôÂàÜ„Å™Á©∫ÁôΩ„ÅÆÂâäÈô§„Å®ÈáçË§áÊéíÈô§\n    metrics = list(set([m.strip() for m in metrics if m.strip()]))\n    dimensions = list(set([d.strip() for d in dimensions if d.strip()]))\n\n    return {\n        'metrics': metrics,\n        'dimensions': dimensions\n    }\n\n# „Åô„Åπ„Å¶„ÅÆ„ÇØ„Ç®„É™Ôºàquery_history_dfÔºâ„Å´ÂØæ„Åó„Å¶„É°„Éà„É™„ÇØ„ÇπÔºè„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„ÇíÊäΩÂá∫„Åô„Çã\nall_metrics = []\nall_dimensions = []\n\nprint(\"üîç Analyzing queries for metrics and dimensions...\")\n\nfor i, row in query_history_df.iterrows():\n    # ÂêÑ„ÇØ„Ç®„É™„ÉÜ„Ç≠„Çπ„Éà„Åã„Çâ„É°„Éà„É™„ÇØ„Çπ„Å®„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„ÇíÊäΩÂá∫\n    analysis = extract_metrics_and_dimensions(row['QUERY_TEXT'])\n    all_metrics.extend(analysis['metrics'])\n    all_dimensions.extend(analysis['dimensions'])\n\n# ÂÖ®„ÇØ„Ç®„É™„Åã„ÇâÈõÜ„ÇÅ„Åü„É°„Éà„É™„ÇØ„ÇπÔºè„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„Çí„É¶„Éã„Éº„ÇØÂåñ\nunique_metrics = list(set(all_metrics))\nunique_dimensions = list(set(all_dimensions))\n\nprint(f\"\\nüìà Analysis Results (with aliases):\")\nprint(f\"   Total unique metrics found: {len(unique_metrics)}\")\nprint(f\"   Total unique dimensions found: {len(unique_dimensions)}\")\n\n# „Çµ„É≥„Éó„É´„Å®„Åó„Å¶ÂÖàÈ†≠5‰ª∂„ÇíË°®Á§∫ÔºàÂà•Âêç‰ªò„Åç„ÅÆ„Åæ„ÅæÔºâ\nif unique_metrics:\n    print(f\"\\nüî¢ Sample Metrics (with aliases):\")\n    for metric in unique_metrics[:5]:\n        print(f\"   - {metric}\")\n\nif unique_dimensions:\n    print(f\"\\nüìä Sample Dimensions (with aliases):\")\n    for dim in unique_dimensions[:5]:\n        print(f\"   - {dim}\")\n\n# VHOL „Åß‰ΩøÁî®„Åó„Å¶„ÅÑ„Çã„ÉÜ„Éº„Éñ„É´„Ç®„Ç§„É™„Ç¢„Çπ‚ÜíÂÆü„ÉÜ„Éº„Éñ„É´Âêç„ÅÆ„Éû„ÉÉ„Éî„É≥„Ç∞\nalias_to_table = {\n    'F': 'HR_EMPLOYEE_FACT',\n    'E': 'EMPLOYEE_DIM', \n    'D': 'DEPARTMENT_DIM',\n    'J': 'JOB_DIM',\n    'L': 'LOCATION_DIM'\n}\n\nprint(f\"\\nüîß Resolving VHOL table aliases to actual table names...\")\nprint(f\"üìã Alias mappings: {alias_to_table}\")\n\n# „É°„Éà„É™„ÇØ„ÇπÂÜÖ„ÅÆ„Ç®„Ç§„É™„Ç¢„ÇπÔºàF., E. „Å™„Å©Ôºâ„ÇíÂÆü„ÉÜ„Éº„Éñ„É´Âêç„Å´ÁΩÆÊèõ\nresolved_metrics = []\nfor metric in unique_metrics:\n    resolved_metric = metric\n    for alias, table in alias_to_table.items():\n        resolved_metric = resolved_metric.replace(f'{alias}.', f'{table}.')\n    resolved_metrics.append(resolved_metric)\n\n# „Éá„Ç£„É°„É≥„Ç∑„Éß„É≥ÂÜÖ„ÅÆ„Ç®„Ç§„É™„Ç¢„ÇπÔºàF.COLUMN „Å™„Å©Ôºâ„ÇíÂÆü„ÉÜ„Éº„Éñ„É´Âêç„Å´ÁΩÆÊèõ\nresolved_dimensions = []\nfor dim in unique_dimensions:\n    if '.' in dim:\n        table_alias = dim.split('.')[0]\n        column_name = dim.split('.')[1]\n\n        # „Ç®„Ç§„É™„Ç¢„Çπ„Åå„Éû„ÉÉ„Éî„É≥„Ç∞„ÉÜ„Éº„Éñ„É´„Å´Â≠òÂú®„Åô„ÇãÂ†¥Âêà„ÅÆ„ÅøÁΩÆÊèõ\n        if table_alias in alias_to_table:\n            resolved_dim = f\"{alias_to_table[table_alias]}.{column_name}\"\n            resolved_dimensions.append(resolved_dim)\n        else:\n            # „Éû„ÉÉ„Éî„É≥„Ç∞„Å´„Å™„ÅÑ„ÇÇ„ÅÆ„ÅØ„Åù„ÅÆ„Åæ„Åæ\n            resolved_dimensions.append(dim)\n    else:\n        # table.column ÂΩ¢Âºè„Åß„Å™„ÅÑ„ÇÇ„ÅÆ„ÅØ„Åù„ÅÆ„Åæ„Åæ\n        resolved_dimensions.append(dim)\n\n# ÁΩÆÊèõÂæå„ÅÆ„É°„Éà„É™„ÇØ„ÇπÔºè„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„Çí„É¶„Éã„Éº„ÇØÂåñ„Åó„Å¶ÊúÄÁµÇÁµêÊûú„Å®„Åô„Çã\nunique_metrics = list(set(resolved_metrics))\nunique_dimensions = list(set(resolved_dimensions))\n\nprint(f\"\\n‚úÖ Final Analysis Results (aliases resolved):\")\nprint(f\"   üìä Resolved metrics: {len(unique_metrics)}\")\nprint(f\"   üìè Resolved dimensions: {len(unique_dimensions)}\")\n\n# ÁΩÆÊèõÂæå„ÅÆ„Çµ„É≥„Éó„É´„ÇíË°®Á§∫ÔºàÂÖàÈ†≠5‰ª∂Ôºâ\nif unique_metrics:\n    print(f\"\\nüî¢ Final Resolved Metrics:\")\n    for metric in unique_metrics[:5]:\n        print(f\"   - {metric}\")\n\nif unique_dimensions:\n    print(f\"\\nüìä Final Resolved Dimensions:\")\n    for dim in unique_dimensions[:5]:\n        print(f\"   - {dim}\")\n\nprint(f\"\\nüéØ Ready for semantic view enhancement!\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "26cc6270-f11c-4aab-8dfc-9daee9a61127",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "### „Åì„ÅÆ„Çª„É´„Åß„ÅØ„ÄÅÂØæË±°„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº `HR_SEMANTIC_VIEW` „ÅÆÁèæÂú®„ÅÆ DDL „Çí Snowflake „Åã„ÇâÂèñÂæó„Åó„ÄÅÂÜÖÂÆπ„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ\n\n- `SELECT GET_DDL('semantic_view', '{SEMANTIC_VIEW_NAME}')` „ÇíÂÆüË°å„Åó„ÄÅÁµêÊûú„Çí `current_ddl` „Å´Ê†ºÁ¥ç\n- DDL „ÅÆÊñáÂ≠óÊï∞„Å®Ë°åÊï∞„ÄÅÂÖàÈ†≠ 20 Ë°åÂàÜ„ÇíË°®Á§∫„Åó„ÄÅ„Å©„ÅÆ„Çà„ÅÜ„Å™ÂÆöÁæ©„Å´„Å™„Å£„Å¶„ÅÑ„Çã„Åã„Çí„Åñ„Å£„Åè„ÇäÁ¢∫Ë™ç\n- ÂèñÂæó„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØ„Ç®„É©„Éº„ÇíË°®Á§∫„Åó„ÄÅ`current_ddl` „ÇíÁ©∫ÊñáÂ≠ó„Å´Ë®≠ÂÆö„Åó„Åæ„Åô„ÄÇ\n\n„Åì„ÅÆ DDL „ÅåÂæåÁ∂ö„ÅÆ Cortex COMPLETE „Å´Ê∏°„Åô„ÄåÂÖÉ„ÅÆ„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„ÉºÂÆöÁæ©„Äç„Å®„Å™„Çä„Åæ„Åô„ÄÇ"
  },
  {
   "cell_type": "code",
   "id": "d42480f9-5eb7-46c2-af72-6bc8cc664f9a",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# Retrieve current semantic view DDL\nprint(f\"üìã Retrieving DDL for {SEMANTIC_VIEW_NAME}...\")\n\ntry:\n    ddl_result = session.sql(f\"SELECT GET_DDL('semantic_view','{SEMANTIC_VIEW_NAME}') as DDL\").collect()\n    \n    if ddl_result and len(ddl_result) > 0:\n        current_ddl = ddl_result[0]['DDL']\n        print(f\"‚úÖ Retrieved DDL for {SEMANTIC_VIEW_NAME}\")\n        print(f\"üìù DDL Length: {len(current_ddl)} characters\")\n        \n        # Show first few lines\n        ddl_lines = current_ddl.split('\\n')\n        print(f\"\\nüìã Preview (first 20 lines):\")\n        for i, line in enumerate(ddl_lines[:50]):\n            print(f\"   {i+1:2d}: {line}\")\n        \n        if len(ddl_lines) > 20:\n            print(f\"   ... ({len(ddl_lines)-20} more lines)\")\n    else:\n        print(f\"‚ùå No DDL found for {SEMANTIC_VIEW_NAME}\")\n        current_ddl = \"\"\n        \nexcept Exception as e:\n    print(f\"‚ùå Error retrieving DDL: {e}\")\n    current_ddl = \"\"\n\nif current_ddl:\n    print(f\"\\n‚úÖ DDL retrieval successful! Ready for AI enhancement.\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  No DDL available - you may need to create the semantic view first.\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eff617bb-dfc1-4c45-8b3a-1af11f11a83c",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "### „Åì„ÅÆ„Çª„É´„Åß„ÅØ„ÄÅÊäΩÂá∫„Åó„Åü„É°„Éà„É™„ÇØ„ÇπÔºè„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥ÂÄôË£ú„Å®Êó¢Â≠ò DDL „Çí„ÇÇ„Å®„Å´„ÄÅCortex „É¢„Éá„É´„Å´„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„ÅÆÊã°ÂºµÊ°à„ÇíÁîüÊàê„Åï„Åõ„Åæ„Åô„ÄÇ\n\n- `unique_metrics` / `unique_dimensions` „ÅÆ‰∏ä‰Ωç 10 ‰ª∂„ÇíÂèñ„ÇäÂá∫„Åó„ÄÅ„Éó„É≠„É≥„Éó„Éà„Å´Âüã„ÇÅËæº„Åø\n- Êó•Êú¨Ë™û„ÅßË©≥Á¥∞„Å™„Éó„É≠„É≥„Éó„Éà„ÇíÊßãÁØâ„Åó„ÄÅ\n  - Êó¢Â≠ò„ÅÆÂÆöÁæ©„ÅØÂ§âÊõ¥„Åó„Å™„ÅÑ\n  - FACTS ‚Üí DIMENSIONS ‚Üí METRICS „ÅÆÈ†ÜÂ∫è„ÇíÂÆà„Çã\n  - ÈõÜË®àÂºè„ÅØ METRICS() „ÅÆ„Åø\n  - ËøΩÂä†Ë°å„Å´„ÅØ `--- added with AI enhancement` „ÅÆ„Ç≥„É°„É≥„Éà„Çí‰ªò‰∏é\n  - WITH EXTENSION „ÅØÂá∫Âäõ„Åó„Å™„ÅÑ\n  „Å®„ÅÑ„Å£„Åü„É´„Éº„É´„ÇíÊòéÁ§∫\n- `SNOWFLAKE.CORTEX.COMPLETE` „ÇíÂëº„Å≥Âá∫„Åó„ÄÅÊã°ÂºµÂæå DDL (`enhanced_ddl`) „ÇíÂèñÂæó\n- ÂÖÉ DDL „Å®„ÅÆË°åÊï∞ÔºèÊñáÂ≠óÊï∞„ÅÆÂ∑ÆÂàÜ„ÄÅËøΩÂä†„Åï„Çå„ÅüË°åÊï∞„Çí„É≠„Ç∞„Å´Âá∫Âäõ„Åó„ÄÅAI „Å´„Çà„ÇãÊã°Âºµ„ÅÆË¶èÊ®°„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ"
  },
  {
   "cell_type": "code",
   "id": "aaaf25a3-e1b2-4953-bf3e-14beb3799c9f",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "if current_ddl and (unique_metrics or unique_dimensions):\n    # Create AI prompt for enhancement (optimized for token efficiency)\n    top_metrics = unique_metrics[:10]  # Top 10 most important\n    top_dimensions = unique_dimensions[:10]  # Top 10 most important\n    \n    prompt = f\"\"\"\nCREATE SEMANTIC VIEW„ÅÆDDL„Åß„ÅÇ„Çã„ÄêÁèæÂú®„ÅÆDDL„Äë„Åã„ÇâÊ§úÂá∫„Åï„Çå„Åü„ÇØ„Ç®„É™„Éë„Çø„Éº„É≥„Å´ÂØæ„Åó„Å¶„ÄÅ\n„ÄêËøΩÂä†„Åó„Åü„ÅÑ„É°„Éà„É™„ÇØ„Çπ„Äë„ÄÅ„ÄêËøΩÂä†„Åó„Åü„ÅÑ„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„Äë„Åã„ÇâÂÖ®„Å¶„ÅÆËøΩÂä†„Åô„Åπ„ÅçÂÜÖÂÆπ„ÇíÂèñÂæó„Åó„Å¶„ÄÅ\nÊñ∞„Åó„ÅÑMETRICS„Å®DIMENSIONS„ÅÆÂÆöÁæ©„Å´ÂÖ®„Å¶ËøΩÂä†„Åô„Çã„Åì„Å®„ÅßÊã°Âºµ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n„ÄêÁèæÂú®„ÅÆDDL„Äë\n{current_ddl}\n\n„ÄêËøΩÂä†„Åó„Åü„ÅÑ„É°„Éà„É™„ÇØ„Çπ„Äë\n{', '.join(top_metrics)}\n\n„ÄêËøΩÂä†„Åó„Åü„ÅÑ„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„Äë\n{', '.join(top_dimensions)}\n\n„Äê„É´„Éº„É´„Äë\n- Êó¢Â≠ò„ÅÆÂÜÖÂÆπ„ÅØ‰∏ÄÂàáÂ§âÊõ¥„Åõ„Åö„ÄÅ„Åù„ÅÆ„Åæ„ÅæÊÆã„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n- ÈáçË¶Å:DDLÂÜÖ„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥È†ÜÂ∫è„ÅØÂøÖ„Åö FACTS(), DIMENSIONS(), METRICS()„ÅÆÈ†Ü„Å´‰øù„Å£„Å¶„Åè„Å†„Åï„ÅÑ\n- „Åô„Åπ„Å¶„ÅÆÈõÜË®àÂºèÔºàSUM, COUNT, AVG „Å™„Å©Ôºâ„ÅØMETRICS()„Çª„ÇØ„Ç∑„Éß„É≥„ÅÆ„Åø„Å´ËøΩÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n- FACTS()„Çª„ÇØ„Ç∑„Éß„É≥„ÅØ„ÉÜ„Éº„Éñ„É´ÂèÇÁÖßÂ∞ÇÁî®„Åß„ÅÇ„Çä„ÄÅÈõÜË®àÂºè„ÅØÂê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ\n- ÈõÜË®à„Çí‰º¥„Çè„Å™„ÅÑÂàóÂèÇÁÖß„ÅØ DIMENSIONS„Çª„ÇØ„Ç∑„Éß„É≥„Å´ËøΩÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n- METRICS()„Çª„ÇØ„Ç∑„Éß„É≥„Åß„ÅÆÊõ∏Âºè„ÅØ\n  table_name.metric_name AS AGG(expression) --- added with AI enhancement\n  „Å®„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n- Âá∫Âäõ„Å´„ÅØ \"WITH EXTENSION\" „Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ\n- ËøΩÂä†„Åó„ÅüË°å„Å´„ÅØÂøÖ„Åö„ÄÅMETRICS / DIMENSIONS„Å©„Å°„Çâ„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„Å©„Å°„Çâ„Åß„ÇÇ\n    --- added with AI enhancement\n  „Å®„ÅÑ„ÅÜ„Ç≥„É°„É≥„Éà„Çí‰ªò„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n- Ëß£Ë™¨Êñá„ÇÑË™¨ÊòéÊñá„ÅØ‰∏ÄÂàáÂá∫Âäõ„Åõ„Åö„ÄÅ„ÄåÂÆåÊàê„Åó„ÅüÊã°ÂºµÂæå„ÅÆ DDL ÂÖ®‰Ωì„ÅÆ„Åø„Äç„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n- Âßã„Åæ„Çä„ÅÆ„Äå``` sql„Äç„ÇÑÁµÇ„Çè„Çä„ÅÆ„Äå``` „Äç„ÅØ‰∏çË¶Å„Åß„Åô„ÄÇ\n- ÁîüÊàê„Åó„ÅüDDL„ÅÆ‰∏≠„Å´ÂÖ®Ëßí„Å™„Å©„ÅåÂê´„Åæ„Çå„Å™„ÅÑ„Çà„ÅÜ„Å´„Åô„Çã„Åì„Å®\n- „Ç´„É≥„Éû„Åå„Ç≥„É°„É≥„Éà„ÅÆ‰∏≠„Å´Âê´„Åæ„Çå„Çã„Å®„Ç®„É©„Éº„Å´„Å™„Å£„Å¶„Åó„Åæ„ÅÜ„Åü„ÇÅ„ÄÅ„Åù„ÅÆÊßò„Å´„Å™„Çâ„Å™„ÅÑ„Çà„ÅÜ„Å´Ê∞ó„Çí„Å§„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n„ÄÄ„Ç´„É≥„Éû„ÅåÂøÖË¶Å„Å™Â†¥Âêà„ÅØ„ÄÅ--- added with AI enhancement„ÅÆÂâç„Å´ÂøÖ„Åö„Å§„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n „ÄÄ‰æãÔºö„ÄåHR_EMPLOYEE_FACT.avg_salary as AVG(SALARY) --- added with AI enhancement,„Äç„ÅØË™§„Çä„Åß„Åô„ÄÇ\n  „ÄÄ„ÄåHR_EMPLOYEE_FACT.avg_salary as AVG(SALARY), --- added with AI enhancement] \n\n„ÄêÊ≠£„Åó„ÅÑ DDL ÊßãÈÄ†„ÅÆ‰æã„Äë\nFACTS (table_references)\nDIMENSIONS (column_references)\nMETRICS (\nHR_EMPLOYEE_FACT.total_salary AS SUM(salary) --- added with AI enhancement\n)\n\n„ÄêÂá∫Âäõ„Äë\nÊã°ÂºµÂæå„ÅÆ CREATE SEMANTIC VIEW DDL ÂÖ®‰Ωì„Çí„Åù„ÅÆ„Åæ„ÅæÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\"\"\"\n\n    \n    # Escape single quotes for SQL\n    prompt_escaped = prompt.replace(\"'\", \"''\")\n    \n    # Use CORTEX_COMPLETE to generate enhanced DDL\n    cortex_sql = f\"\"\"\n    SELECT SNOWFLAKE.CORTEX.COMPLETE(\n        '{CORTEX_MODEL}',\n        '{prompt_escaped}'\n    ) as enhanced_ddl\n    \"\"\"\n    \n    print(f\"ü§ñ Using CORTEX_COMPLETE with {CORTEX_MODEL} to enhance semantic view...\")\n    print(\"   This may take 30-60 seconds...\")\n    \n    try:\n        # Execute CORTEX_COMPLETE\n        cortex_result = session.sql(cortex_sql).collect()\n        \n        if cortex_result and len(cortex_result) > 0:\n            enhanced_ddl = cortex_result[0]['ENHANCED_DDL']\n            print(\"\\n‚úÖ Successfully generated enhanced semantic view DDL!\")\n            \n            # Show statistics\n            original_lines = len(current_ddl.split('\\n'))\n            enhanced_lines = len(enhanced_ddl.split('\\n'))\n            \n            print(f\"üìä Enhancement Statistics:\")\n            print(f\"   Original DDL: {original_lines} lines, {len(current_ddl)} characters\")\n            print(f\"   Enhanced DDL: {enhanced_lines} lines, {len(enhanced_ddl)} characters\")\n            print(f\"   Lines added: {enhanced_lines - original_lines}\")\n            \n            # Count new metrics and dimensions by looking for AI enhancement comments\n            ai_additions_count = enhanced_ddl.count('--- added with AI enhancement')\n            \n            print(f\"   New metrics/dimensions added: {ai_additions_count}\")\n            \n        else:\n            print(\"‚ùå CORTEX_COMPLETE returned no result\")\n            enhanced_ddl = current_ddl\n            \n    except Exception as e:\n        print(f\"‚ùå Error with CORTEX_COMPLETE: {e}\")\n        enhanced_ddl = current_ddl\n        \nelse:\n    print(\"‚ö†Ô∏è  Skipping enhancement - no DDL or no new metrics/dimensions found\")\n    enhanced_ddl = current_ddl if 'current_ddl' in locals() else \"\"\n\n# Display enhanced DDL results\nif 'enhanced_ddl' in locals() and enhanced_ddl:\n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPLETE ENHANCED SEMANTIC VIEW DDL\")\n    print(\"=\"*80)\n    print(\"üìù COMPLETE DDL OUTPUT (no truncation):\")\n    print()\n    print(enhanced_ddl)\n    print()\n    print(\"=\"*80)\n    \n    # Highlight the new AI-enhanced additions\n    enhanced_lines = enhanced_ddl.split('\\n')\n    new_additions = [line for line in enhanced_lines if '--- added with AI enhancement' in line]\n    \n    if new_additions:\n        print(\"\\nü§ñ AI-ENHANCED ADDITIONS:\")\n        print(\"-\" * 50)\n        for addition in new_additions:\n            print(addition.strip())\n    else:\n        print(\"\\n‚ö†Ô∏è  No new additions detected in the enhanced DDL\")\n    \n    print(f\"\\nüí° Next Steps:\")\n    print(f\"   1. Review the enhanced DDL above\")\n    print(f\"   2. Test the DDL in a development environment\")\n    print(f\"   3. Deploy to production when ready\")\n    print(f\"   4. Update documentation with new metrics/dimensions\")\n    \nelse:\n    print(\"‚ùå No enhanced DDL available\")\n\nprint(\"\\nüéâ Analysis complete!\")\nif 'query_history_df' in locals():\n    print(f\"   ‚Ä¢ Analyzed {len(query_history_df)} queries from the last {HOURS_BACK} hours\")\nif 'unique_metrics' in locals():\n    print(f\"   ‚Ä¢ Found {len(unique_metrics)} unique metrics\")\nif 'unique_dimensions' in locals():\n    print(f\"   ‚Ä¢ Found {len(unique_dimensions)} unique dimensions\")\nprint(f\"   ‚Ä¢ Enhanced {SEMANTIC_VIEW_NAME} using {CORTEX_MODEL}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22116a54-4e64-45fb-95cb-ab0f1d861c73",
   "metadata": {
    "language": "sql",
    "name": "cell22"
   },
   "outputs": [],
   "source": "create or replace semantic view HR_SEMANTIC_VIEW  \n\ttables (  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.LOCATION_DIM primary key (LOCATION_KEY) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØÊã†ÁÇπ„ÅÆË®òÈå≤„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØË≠òÂà•Â≠ê„Å®ÂêçÂâç„ÇíÊåÅ„Å§ÂÄãÂà•„ÅÆÊã†ÁÇπ„ÇíË°®„Åó„Åæ„Åô„ÄÇ',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.JOB_DIM primary key (JOB_KEY) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØËÅ∑‰Ωç„Å®„Åù„ÅÆÂàÜÈ°û„ÅÆË®òÈå≤„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØÁµÑÁπîÂÜÖ„ÅÆÂõ∫Êúâ„ÅÆËÅ∑Âêç„Å®„Åù„ÅÆÈöéÂ±§„É¨„Éô„É´„ÇíË°®„Åó„Åæ„Åô„ÄÇ',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.DEPARTMENT_DIM primary key (DEPARTMENT_KEY) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØÈÉ®ÈñÄ„ÅÆÂèÇÁÖßÊÉÖÂ†±„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØ1„Å§„ÅÆÈÉ®ÈñÄ„Å®„Åù„ÅÆË≠òÂà•ÊÉÖÂ†±„ÇíË°®„Åó„Åæ„Åô„ÄÇ',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.EMPLOYEE_DIM primary key (EMPLOYEE_KEY) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØÂæìÊ•≠Âì°„Å®„Åù„ÅÆÂü∫Êú¨„Éó„É≠„Éï„Ç£„Éº„É´ÊÉÖÂ†±„ÅÆË®òÈå≤„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØ1‰∫∫„ÅÆÂæìÊ•≠Âì°„ÇíË°®„Åó„ÄÅ‰∫∫Âè£Áµ±Ë®à„ÅÆË©≥Á¥∞„Å®ÈõáÁî®Êó•„ÇíÂê´„Åø„Åæ„Åô„ÄÇ',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.HR_EMPLOYEE_FACT primary key (HR_FACT_ID) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØÁµ¶‰∏é„Å®ÈÄÄËÅ∑ÊÉÖÂ†±„ÇíÂê´„ÇÄÂæìÊ•≠Âì°„ÅÆË®òÈå≤„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØÁâπÂÆö„ÅÆÊôÇÁÇπ„Å´„Åä„Åë„ÇãÂæìÊ•≠Âì°„ÅÆÁä∂Ê≥Å„ÇíË°®„Åó„ÄÅÁµÑÁπîÂÜÖ„ÅÆÈÖçÁΩÆ„ÄÅÂ†±ÈÖ¨„ÄÅ„Åä„Çà„Å≥ÁµÑÁπî„ÇíÂéª„Å£„Åü„Åã„Å©„ÅÜ„Åã„ÅÆÊÉÖÂ†±„ÇíÂê´„Åø„Åæ„Åô„ÄÇ'  \n\t)  \n\trelationships (  \n\t\tHR_EMPLOYEE_FACT_TO_DEPARTMENT_DIM as HR_EMPLOYEE_FACT(DEPARTMENT_KEY) references DEPARTMENT_DIM(DEPARTMENT_KEY),  \n\t\tHR_EMPLOYEE_FACT_TO_EMPLOYEE_DIM as HR_EMPLOYEE_FACT(EMPLOYEE_KEY) references EMPLOYEE_DIM(EMPLOYEE_KEY),  \n\t\tHR_EMPLOYEE_FACT_TO_JOB_DIM as HR_EMPLOYEE_FACT(JOB_KEY) references JOB_DIM(JOB_KEY),  \n\t\tHR_EMPLOYEE_FACT_TO_LOCATION_DIM as HR_EMPLOYEE_FACT(LOCATION_KEY) references LOCATION_DIM(LOCATION_KEY)  \n\t)  \n\tfacts (  \n\t\tHR_EMPLOYEE_FACT.SALARY as SALARY comment='Á±≥„Éâ„É´„Åß„ÅÆÂπ¥ÈñìÂü∫Êú¨Áµ¶‰∏é„ÄÇ'  \n\t)  \n\tdimensions (  \n\t\tLOCATION_DIM.LOCATION_KEY as LOCATION_KEY comment='ÂêÑÊã†ÁÇπ„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tLOCATION_DIM.LOCATION_NAME as LOCATION_NAME comment='„Ç¢„É°„É™„Ç´ÂêàË°ÜÂõΩÂÜÖ„ÅÆÂ∏Ç„Å®Â∑û„ÅÆÂêçÁß∞„ÄÇ',  \n\t\tJOB_DIM.JOB_KEY as JOB_KEY comment='ÂêÑËÅ∑‰Ωç„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tJOB_DIM.JOB_LEVEL as JOB_LEVEL comment='ÁµÑÁπîÂÜÖ„Åß„ÅÆËÅ∑‰Ωç„ÅÆÈöéÂ±§„É¨„Éô„É´„Åæ„Åü„ÅØ„É©„É≥„ÇØ„ÄÇ',  \n\t\tJOB_DIM.JOB_TITLE as JOB_TITLE comment='ÁµÑÁπîÂÜÖ„Åß„ÅÆÂ∞ÇÈñÄÁöÑ„Å™ËÅ∑‰Ωç„Åæ„Åü„ÅØÂΩπÂâ≤„ÄÇ',  \n\t\tDEPARTMENT_DIM.DEPARTMENT_KEY as DEPARTMENT_KEY comment='ÂêÑÈÉ®ÈñÄ„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tDEPARTMENT_DIM.DEPARTMENT_NAME as DEPARTMENT_NAME comment='ÁµÑÁπîÂÜÖ„ÅÆ„Éì„Ç∏„Éç„ÇπÈÉ®ÈñÄ„Åæ„Åü„ÅØÊ©üËÉΩÂçò‰Ωç„ÅÆÂêçÁß∞„ÄÇ',  \n\t\tEMPLOYEE_DIM.EMPLOYEE_KEY as EMPLOYEE_KEY comment='ÂêÑÂæìÊ•≠Âì°„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tEMPLOYEE_DIM.EMPLOYEE_NAME as EMPLOYEE_NAME comment='ÂæìÊ•≠Âì°„ÅÆ„Éï„É´„Éç„Éº„É†„ÄÇ',  \n\t\tEMPLOYEE_DIM.GENDER as GENDER comment='ÂæìÊ•≠Âì°„ÅÆÁîüÁâ©Â≠¶ÁöÑÊÄßÂà•„ÄÇ',  \n\t\tEMPLOYEE_DIM.HIRE_DATE as HIRE_DATE comment='ÂæìÊ•≠Âì°„ÅåÈõáÁî®„Åï„Çå„ÅüÊó•‰ªò„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.ATTRITION_FLAG as ATTRITION_FLAG comment='ÂæìÊ•≠Âì°„ÅåÁµÑÁπî„ÇíÂéª„Å£„Åü„Åã„Å©„ÅÜ„Åã„ÇíÁ§∫„Åô„Éê„Ç§„Éä„É™ÊåáÊ®ô„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.DEPARTMENT_KEY as DEPARTMENT_KEY comment='ÁµÑÁπîÂÜÖ„ÅÆÈÉ®ÈñÄ„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.EMPLOYEE_KEY as EMPLOYEE_KEY comment='ÂêÑÂæìÊ•≠Âì°„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.HR_FACT_ID as HR_FACT_ID comment='ÂêÑ‰∫∫‰∫ãË®òÈå≤„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.JOB_KEY as JOB_KEY comment='ÁµÑÁπîÂÜÖ„ÅÆËÅ∑‰Ωç„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.LOCATION_KEY as LOCATION_KEY comment='ÂæìÊ•≠Âì°„ÅÆÂã§ÂãôÂú∞„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.DATE as DATE comment='ÂæìÊ•≠Âì°„ÅÆË®òÈå≤„Åå‰ΩúÊàê„Åæ„Åü„ÅØÊõ¥Êñ∞„Åï„Çå„ÅüÊó•‰ªò„ÄÇ'  \n\t)  \n\tmetrics (  \n\t\tHR_EMPLOYEE_FACT.avg_salary AS AVG(SALARY), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.total_attrition AS SUM(ATTRITION_FLAG), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.female_employee_count AS COUNT(DISTINCT CASE WHEN EMPLOYEE_DIM.GENDER = 'F' THEN HR_EMPLOYEE_FACT.EMPLOYEE_KEY END), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.department_count AS COUNT(DISTINCT DEPARTMENT_KEY), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.max_salary AS MAX(SALARY), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.total_employees AS COUNT(DISTINCT EMPLOYEE_KEY), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.salary_percentile_75 AS PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY SALARY) --- added with AI enhancement  \n\t)  ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f09a1e45-bf71-4c7b-8482-93f4336e009c",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "//„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„ÉóSQL\n\ncreate or replace semantic view HR_SEMANTIC_VIEW  \n\ttables (  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.LOCATION_DIM primary key (LOCATION_KEY) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØÊã†ÁÇπ„ÅÆË®òÈå≤„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØË≠òÂà•Â≠ê„Å®ÂêçÂâç„ÇíÊåÅ„Å§ÂÄãÂà•„ÅÆÊã†ÁÇπ„ÇíË°®„Åó„Åæ„Åô„ÄÇ',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.JOB_DIM primary key (JOB_KEY) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØËÅ∑‰Ωç„Å®„Åù„ÅÆÂàÜÈ°û„ÅÆË®òÈå≤„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØÁµÑÁπîÂÜÖ„ÅÆÂõ∫Êúâ„ÅÆËÅ∑Âêç„Å®„Åù„ÅÆÈöéÂ±§„É¨„Éô„É´„ÇíË°®„Åó„Åæ„Åô„ÄÇ',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.DEPARTMENT_DIM primary key (DEPARTMENT_KEY) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØÈÉ®ÈñÄ„ÅÆÂèÇÁÖßÊÉÖÂ†±„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØ1„Å§„ÅÆÈÉ®ÈñÄ„Å®„Åù„ÅÆË≠òÂà•ÊÉÖÂ†±„ÇíË°®„Åó„Åæ„Åô„ÄÇ',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.EMPLOYEE_DIM primary key (EMPLOYEE_KEY) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØÂæìÊ•≠Âì°„Å®„Åù„ÅÆÂü∫Êú¨„Éó„É≠„Éï„Ç£„Éº„É´ÊÉÖÂ†±„ÅÆË®òÈå≤„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØ1‰∫∫„ÅÆÂæìÊ•≠Âì°„ÇíË°®„Åó„ÄÅ‰∫∫Âè£Áµ±Ë®à„ÅÆË©≥Á¥∞„Å®ÈõáÁî®Êó•„ÇíÂê´„Åø„Åæ„Åô„ÄÇ',  \n\t\tSV_VHOL_DB.VHOL_SCHEMA.HR_EMPLOYEE_FACT primary key (HR_FACT_ID) comment='„Åì„ÅÆ„ÉÜ„Éº„Éñ„É´„Å´„ÅØÁµ¶‰∏é„Å®ÈÄÄËÅ∑ÊÉÖÂ†±„ÇíÂê´„ÇÄÂæìÊ•≠Âì°„ÅÆË®òÈå≤„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂêÑ„É¨„Ç≥„Éº„Éâ„ÅØÁâπÂÆö„ÅÆÊôÇÁÇπ„Å´„Åä„Åë„ÇãÂæìÊ•≠Âì°„ÅÆÁä∂Ê≥Å„ÇíË°®„Åó„ÄÅÁµÑÁπîÂÜÖ„ÅÆÈÖçÁΩÆ„ÄÅÂ†±ÈÖ¨„ÄÅ„Åä„Çà„Å≥ÁµÑÁπî„ÇíÂéª„Å£„Åü„Åã„Å©„ÅÜ„Åã„ÅÆÊÉÖÂ†±„ÇíÂê´„Åø„Åæ„Åô„ÄÇ'  \n\t)  \n\trelationships (  \n\t\tHR_EMPLOYEE_FACT_TO_DEPARTMENT_DIM as HR_EMPLOYEE_FACT(DEPARTMENT_KEY) references DEPARTMENT_DIM(DEPARTMENT_KEY),  \n\t\tHR_EMPLOYEE_FACT_TO_EMPLOYEE_DIM as HR_EMPLOYEE_FACT(EMPLOYEE_KEY) references EMPLOYEE_DIM(EMPLOYEE_KEY),  \n\t\tHR_EMPLOYEE_FACT_TO_JOB_DIM as HR_EMPLOYEE_FACT(JOB_KEY) references JOB_DIM(JOB_KEY),  \n\t\tHR_EMPLOYEE_FACT_TO_LOCATION_DIM as HR_EMPLOYEE_FACT(LOCATION_KEY) references LOCATION_DIM(LOCATION_KEY)  \n\t)  \n\tfacts (  \n\t\tHR_EMPLOYEE_FACT.SALARY as SALARY comment='Á±≥„Éâ„É´„Åß„ÅÆÂπ¥ÈñìÂü∫Êú¨Áµ¶‰∏é„ÄÇ'  \n\t)  \n\tdimensions (  \n\t\tLOCATION_DIM.LOCATION_KEY as LOCATION_KEY comment='ÂêÑÊã†ÁÇπ„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tLOCATION_DIM.LOCATION_NAME as LOCATION_NAME comment='„Ç¢„É°„É™„Ç´ÂêàË°ÜÂõΩÂÜÖ„ÅÆÂ∏Ç„Å®Â∑û„ÅÆÂêçÁß∞„ÄÇ',  \n\t\tJOB_DIM.JOB_KEY as JOB_KEY comment='ÂêÑËÅ∑‰Ωç„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tJOB_DIM.JOB_LEVEL as JOB_LEVEL comment='ÁµÑÁπîÂÜÖ„Åß„ÅÆËÅ∑‰Ωç„ÅÆÈöéÂ±§„É¨„Éô„É´„Åæ„Åü„ÅØ„É©„É≥„ÇØ„ÄÇ',  \n\t\tJOB_DIM.JOB_TITLE as JOB_TITLE comment='ÁµÑÁπîÂÜÖ„Åß„ÅÆÂ∞ÇÈñÄÁöÑ„Å™ËÅ∑‰Ωç„Åæ„Åü„ÅØÂΩπÂâ≤„ÄÇ',  \n\t\tDEPARTMENT_DIM.DEPARTMENT_KEY as DEPARTMENT_KEY comment='ÂêÑÈÉ®ÈñÄ„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tDEPARTMENT_DIM.DEPARTMENT_NAME as DEPARTMENT_NAME comment='ÁµÑÁπîÂÜÖ„ÅÆ„Éì„Ç∏„Éç„ÇπÈÉ®ÈñÄ„Åæ„Åü„ÅØÊ©üËÉΩÂçò‰Ωç„ÅÆÂêçÁß∞„ÄÇ',  \n\t\tEMPLOYEE_DIM.EMPLOYEE_KEY as EMPLOYEE_KEY comment='ÂêÑÂæìÊ•≠Âì°„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tEMPLOYEE_DIM.EMPLOYEE_NAME as EMPLOYEE_NAME comment='ÂæìÊ•≠Âì°„ÅÆ„Éï„É´„Éç„Éº„É†„ÄÇ',  \n\t\tEMPLOYEE_DIM.GENDER as GENDER comment='ÂæìÊ•≠Âì°„ÅÆÁîüÁâ©Â≠¶ÁöÑÊÄßÂà•„ÄÇ',  \n\t\tEMPLOYEE_DIM.HIRE_DATE as HIRE_DATE comment='ÂæìÊ•≠Âì°„ÅåÈõáÁî®„Åï„Çå„ÅüÊó•‰ªò„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.ATTRITION_FLAG as ATTRITION_FLAG comment='ÂæìÊ•≠Âì°„ÅåÁµÑÁπî„ÇíÂéª„Å£„Åü„Åã„Å©„ÅÜ„Åã„ÇíÁ§∫„Åô„Éê„Ç§„Éä„É™ÊåáÊ®ô„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.DEPARTMENT_KEY as DEPARTMENT_KEY comment='ÁµÑÁπîÂÜÖ„ÅÆÈÉ®ÈñÄ„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.EMPLOYEE_KEY as EMPLOYEE_KEY comment='ÂêÑÂæìÊ•≠Âì°„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.HR_FACT_ID as HR_FACT_ID comment='ÂêÑ‰∫∫‰∫ãË®òÈå≤„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.JOB_KEY as JOB_KEY comment='ÁµÑÁπîÂÜÖ„ÅÆËÅ∑‰Ωç„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.LOCATION_KEY as LOCATION_KEY comment='ÂæìÊ•≠Âì°„ÅÆÂã§ÂãôÂú∞„ÅÆ‰∏ÄÊÑè„ÅÆË≠òÂà•Â≠ê„ÄÇ',  \n\t\tHR_EMPLOYEE_FACT.DATE as DATE comment='ÂæìÊ•≠Âì°„ÅÆË®òÈå≤„Åå‰ΩúÊàê„Åæ„Åü„ÅØÊõ¥Êñ∞„Åï„Çå„ÅüÊó•‰ªò„ÄÇ'  \n\t)  \n\tmetrics (  \n\t\tHR_EMPLOYEE_FACT.avg_salary AS AVG(SALARY), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.total_attrition AS SUM(ATTRITION_FLAG), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.female_employee_count AS COUNT(DISTINCT CASE WHEN EMPLOYEE_DIM.GENDER = 'F' THEN HR_EMPLOYEE_FACT.EMPLOYEE_KEY END), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.department_count AS COUNT(DISTINCT DEPARTMENT_KEY), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.max_salary AS MAX(SALARY), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.total_employees AS COUNT(DISTINCT EMPLOYEE_KEY), --- added with AI enhancement  \n\t\tHR_EMPLOYEE_FACT.salary_percentile_75 AS PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY SALARY) --- added with AI enhancement  \n\t)  ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6cf818a-e5da-4650-807c-8a372b7deae6",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "### „Åì„ÅÆ„Çª„É´„Åß„ÅØ„ÄÅÁîüÊàê„Åï„Çå„Åü `enhanced_ddl` „ÇíÂÆüÈöõ„ÅÆ Snowflake „Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Å®„Åó„Å¶ÂèçÊò†„Åó„Åæ„Åô„ÄÇ\n\n- „Åæ„Åö `DROP SEMANTIC VIEW IF EXISTS {SEMANTIC_VIEW_NAME}` „ÅßÊó¢Â≠ò„ÅÆ„Éì„É•„Éº„ÇíÂâäÈô§\n- „Åù„ÅÆÂæå `session.sql(enhanced_ddl)` „ÇíÂÆüË°å„Åó„Å¶„ÄÅÂº∑ÂåñÁâà„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„Çí‰ΩúÊàê\n- `SHOW SEMANTIC VIEWS LIKE '{SEMANTIC_VIEW_NAME}'` „Åß‰ΩúÊàêÁµêÊûú„ÇíÊ§úË®º„Åó„ÄÅÂêçÂâçÔºèDBÔºè„Çπ„Ç≠„Éº„ÉûÔºè‰ΩúÊàêÊó•ÊôÇ„ÇíÂá∫Âäõ\n- DDL ÂÜÖ„ÅÆ `--- added with AI enhancement` „ÅÆÂá∫ÁèæÂõûÊï∞„ÇíÊï∞„Åà„ÄÅAI „Å´„Çà„Å£„Å¶ËøΩÂä†„Åï„Çå„Åü„É°„Éà„É™„ÇØ„Çπ„Éª„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„ÅÆÊï∞„ÇíË¶ÅÁ¥Ñ\n- „Ç®„É©„Éº„ÅåËµ∑„Åç„ÅüÂ†¥Âêà„ÅØ„ÄÅÊ®©Èôê„ÉªDDL ÊñáÊ≥ï„ÉªÂèÇÁÖß„ÉÜ„Éº„Éñ„É´„ÅÆÂ≠òÂú®„Å™„Å©„ÄÅ‰ª£Ë°®ÁöÑ„Å™Á¢∫Ë™ç„Éù„Ç§„É≥„Éà„Çí„Ç¨„Ç§„Éâ„Å®„Åó„Å¶Ë°®Á§∫„Åó„Åæ„Åô„ÄÇ\n\"\"\""
  },
  {
   "cell_type": "code",
   "id": "fe19254a-5556-4374-bc56-9d979153e351",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Deploy the enhanced semantic view DDL\nif 'enhanced_ddl' in locals() and enhanced_ddl and enhanced_ddl.strip():\n    print(\"üöÄ Deploying Enhanced Semantic View...\")\n    print(\"=\"*60)\n    \n    try:\n        # First, drop the existing semantic view if it exists\n        drop_sql = f\"DROP SEMANTIC VIEW IF EXISTS {SEMANTIC_VIEW_NAME}\"\n        print(f\"üìã Dropping existing semantic view: {SEMANTIC_VIEW_NAME}\")\n        session.sql(drop_sql).collect()\n        print(\"   ‚úÖ Existing semantic view dropped successfully\")\n        \n        # Execute the enhanced DDL\n        print(f\"üîß Creating enhanced semantic view...\")\n        session.sql(enhanced_ddl).collect()\n        print(\"   ‚úÖ Enhanced semantic view created successfully!\")\n        \n        # Verify the deployment\n        verification_sql = f\"SHOW SEMANTIC VIEWS LIKE '{SEMANTIC_VIEW_NAME}'\"\n        result = session.sql(verification_sql).collect()\n        \n        if result:\n            print(f\"\\nüéâ SUCCESS! Enhanced {SEMANTIC_VIEW_NAME} deployed successfully!\")\n            print(f\"üìä Semantic view details:\")\n            for row in result:\n                print(f\"   Name: {row['name']}\")\n                print(f\"   Database: {row['database_name']}\")\n                print(f\"   Schema: {row['schema_name']}\")\n                print(f\"   Created: {row['created_on']}\")\n        else:\n            print(f\"‚ö†Ô∏è  Deployment completed but verification failed - please check manually\")\n            \n        # Show what was added\n        if '--- added with AI enhancement' in enhanced_ddl:\n            additions_count = enhanced_ddl.count('--- added with AI enhancement')\n            print(f\"\\nü§ñ AI Enhancement Summary:\")\n            print(f\"   ‚Ä¢ {additions_count} new metrics/dimensions added\")\n            print(f\"   ‚Ä¢ All additions marked with '--- added with AI enhancement'\")\n            print(f\"   ‚Ä¢ Ready for immediate use in analytics!\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error deploying semantic view: {e}\")\n        print(f\"\\nüîç Troubleshooting:\")\n        print(f\"   1. Check if you have CREATE SEMANTIC VIEW privileges\")\n        print(f\"   2. Verify the DDL syntax above is correct\")\n        print(f\"   3. Ensure all referenced tables exist\")\n        print(f\"   4. Try running the DDL manually if needed\")\n        \nelse:\n    print(\"‚ö†Ô∏è  No enhanced DDL available for deployment\")\n    print(\"   Please run Step 5 first to generate the enhanced DDL\")\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"üèÅ SEMANTIC VIEW ENHANCEMENT WORKFLOW COMPLETE!\")\nprint(\"=\"*60)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc4f76d3-db62-42a1-8ba0-eaf5c7fcf6fa",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "### „Åì„ÅÆ„Çª„É´„Åß„ÅØ„ÄÅStreamlit „Çí‰Ωø„Å£„Å¶ `HR_SEMANTIC_VIEW` „ÇíÂèØË¶ñÂåñ„Åô„Çã„Ç§„É≥„Çø„É©„ÇØ„ÉÜ„Ç£„Éñ„Å™„Ç¢„Éó„É™„ÇíÂÆöÁæ©„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n- `SHOW SEMANTIC METRICS IN ...` / `SHOW SEMANTIC DIMENSIONS IN ...` „ÇíÂëº„Å≥Âá∫„Åó„ÄÅÂà©Áî®ÂèØËÉΩ„Å™„É°„Éà„É™„ÇØ„ÇπÔºè„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„ÇíÂãïÁöÑ„Å´ÂèñÂæó\n- „Éó„É´„ÉÄ„Ç¶„É≥„Åß„Äå„É°„Éà„É™„ÇØ„Çπ„Äç„Äå„Éá„Ç£„É°„É≥„Ç∑„Éß„É≥„Äç„ÇíÈÅ∏„Å≥„ÄÅË°åÊï∞„Éª„ÇΩ„Éº„ÉàËª∏„Éª„ÇΩ„Éº„ÉàÊñπÂêë„Éª„Ç∞„É©„ÉïÁ®ÆÂà•ÔºàË°®ÔºèÁ∏¶Ê£íÔºèÊ®™Ê£íÔºèÊäò„ÇåÁ∑öÔºèÂÜÜÔºâ„ÇíÊåáÂÆö\n- `SEMANTIC_VIEW()` Èñ¢Êï∞„Çí‰Ωø„Å£„Åü SQL „ÇíËá™ÂãïÁîüÊàê„Åó„ÄÅÁµêÊûú„Çí DataFrame „Å® Plotly „Ç∞„É©„Éï„ÅßË°®Á§∫\n- ÁîüÊàê„Åï„Çå„Åü SQL „ÇÑ„Ç®„É©„ÉºË©≥Á¥∞„Çí„Ç®„ÇØ„Çπ„Éë„É≥„ÉÄ„Éº„ÅßÁ¢∫Ë™ç„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„ÄÅ„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„ÅÆÊåôÂãï„Çí„Ç§„É≥„Çø„É©„ÇØ„ÉÜ„Ç£„Éñ„Å´Ê§úË®º„Åß„Åç„Åæ„Åô„ÄÇ"
  },
  {
   "cell_type": "code",
   "id": "75654071-2397-4312-bf5b-7d6abf1277d2",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# Interactive Semantic View Visualization - Streamlit App for Snowflake Notebooks\n# Uses SHOW METRICS and SHOW DIMENSIONS to dynamically discover available metrics and dimensions\n# \n# Usage in Snowflake Notebook:\n# 1. Make sure you have created the HR_SEMANTIC_VIEW\n# 2. Paste this code into a Streamlit cell\n# 3. The app will automatically discover metrics and dimensions\n\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\n\n# Semantic view configuration - adjust if needed\nSEMANTIC_VIEW_NAME = \"HR_SEMANTIC_VIEW\"\nSEMANTIC_VIEW_SCHEMA = \"SV_VHOL_DB.VHOL_SCHEMA\"  # Full schema path\nSEMANTIC_VIEW_FULL_NAME = f\"{SEMANTIC_VIEW_SCHEMA}.{SEMANTIC_VIEW_NAME}\"\n\ndef main():\n    st.title(\"üéØ Semantic View Interactive Visualization\")\n    st.markdown(f\"**Semantic View:** `{SEMANTIC_VIEW_FULL_NAME}`\")\n    \n    # Check if session is available (Snowflake notebook context)\n    if 'session' not in globals():\n        st.error(\"‚ùå Snowflake session not available. Please run this in a Snowflake notebook.\")\n        st.info(\"üí° Make sure you're running this in a Snowflake notebook with `session` available\")\n        return\n    \n    # Extract available metrics and dimensions using SHOW commands\n    @st.cache_data\n    def get_options():\n        \"\"\"Get metrics and dimensions from semantic view using SHOW SEMANTIC METRICS/DIMENSIONS commands\n        Returns: (metrics_list, dimensions_list, metrics_map, dimensions_map)\n        where maps contain full_name -> short_name mappings\n        \"\"\"\n        metrics = []\n        dimensions = []\n        metrics_map = {}  # full_name -> short_name\n        dimensions_map = {}  # full_name -> short_name\n        \n        try:\n            # Get metrics from semantic view\n            show_metrics_sql = f\"SHOW SEMANTIC METRICS IN {SEMANTIC_VIEW_FULL_NAME}\"\n            \n            with st.spinner(\"üîç Fetching metrics from semantic view...\"):\n                metrics_result = session.sql(show_metrics_sql).collect()\n            \n            if metrics_result and len(metrics_result) > 0:\n                # Convert to DataFrame to inspect structure\n                metrics_df = pd.DataFrame([dict(row.asDict()) for row in metrics_result])\n                \n                # Debug: Show available columns (first time only)\n                if 'metrics_debug' not in st.session_state:\n                    with st.expander(\"üîç Metrics Result Structure (Debug)\", expanded=False):\n                        st.dataframe(metrics_df.head())\n                        st.write(f\"Columns: {list(metrics_df.columns)}\")\n                    st.session_state.metrics_debug = True\n                \n                # Extract metric names - try common column names\n                metric_name_col = None\n                table_name_col = None\n                \n                for col in ['name', 'metric_name', 'metric', 'METRIC_NAME', 'NAME']:\n                    if col in metrics_df.columns:\n                        metric_name_col = col\n                        break\n                \n                # Try to find table name column\n                for col in ['table_name', 'table', 'TABLE_NAME', 'TABLE', 'source_table', 'entity_name']:\n                    if col in metrics_df.columns:\n                        table_name_col = col\n                        break\n                \n                if metric_name_col:\n                    for _, row in metrics_df.iterrows():\n                        metric_name = str(row[metric_name_col]).strip()\n                        if pd.isna(metric_name) or not metric_name:\n                            continue\n                        \n                        # Try to get table name\n                        table_name = None\n                        if table_name_col and table_name_col in row:\n                            table_name = str(row[table_name_col]).strip()\n                            if pd.isna(table_name) or not table_name:\n                                table_name = None\n                        \n                        # Check if metric_name already contains table prefix (table.metric format)\n                        if '.' in metric_name:\n                            # Already has table prefix\n                            full_name = metric_name\n                            short_name = metric_name.split('.')[-1]\n                            metrics.append(full_name)\n                            metrics_map[full_name] = short_name\n                        elif table_name:\n                            # Create full name with table prefix\n                            full_name = f\"{table_name}.{metric_name}\"\n                            metrics.append(full_name)\n                            metrics_map[full_name] = metric_name\n                        else:\n                            # If no table name, use just the metric name\n                            metrics.append(metric_name)\n                            metrics_map[metric_name] = metric_name\n                else:\n                    # Fallback: use first column\n                    metrics_raw = metrics_df.iloc[:, 0].dropna().unique().tolist()\n                    for metric in metrics_raw:\n                        metrics.append(str(metric))\n                        metrics_map[str(metric)] = str(metric)\n            else:\n                st.warning(\"‚ö†Ô∏è No metrics found in semantic view\")\n            \n            # Get dimensions from semantic view\n            show_dimensions_sql = f\"SHOW SEMANTIC DIMENSIONS IN {SEMANTIC_VIEW_FULL_NAME}\"\n            \n            with st.spinner(\"üîç Fetching dimensions from semantic view...\"):\n                dimensions_result = session.sql(show_dimensions_sql).collect()\n            \n            if dimensions_result and len(dimensions_result) > 0:\n                # Convert to DataFrame to inspect structure\n                dimensions_df = pd.DataFrame([dict(row.asDict()) for row in dimensions_result])\n                \n                # Debug: Show available columns (first time only)\n                if 'dimensions_debug' not in st.session_state:\n                    with st.expander(\"üîç Dimensions Result Structure (Debug)\", expanded=False):\n                        st.dataframe(dimensions_df.head())\n                        st.write(f\"Columns: {list(dimensions_df.columns)}\")\n                    st.session_state.dimensions_debug = True\n                \n                # Extract dimension names - try common column names\n                dimension_name_col = None\n                table_name_col = None\n                \n                for col in ['name', 'dimension_name', 'dimension', 'DIMENSION_NAME', 'NAME']:\n                    if col in dimensions_df.columns:\n                        dimension_name_col = col\n                        break\n                \n                # Try to find table name column\n                for col in ['table_name', 'table', 'TABLE_NAME', 'TABLE', 'source_table', 'entity_name']:\n                    if col in dimensions_df.columns:\n                        table_name_col = col\n                        break\n                \n                if dimension_name_col:\n                    for _, row in dimensions_df.iterrows():\n                        dimension_name = str(row[dimension_name_col]).strip()\n                        if pd.isna(dimension_name) or not dimension_name:\n                            continue\n                        \n                        # Try to get table name\n                        table_name = None\n                        if table_name_col and table_name_col in row:\n                            table_name = str(row[table_name_col]).strip()\n                            if pd.isna(table_name) or not table_name:\n                                table_name = None\n                        \n                        # Check if dimension_name already contains table prefix (table.dimension format)\n                        if '.' in dimension_name:\n                            # Already has table prefix\n                            full_name = dimension_name\n                            short_name = dimension_name.split('.')[-1]\n                            dimensions.append(full_name)\n                            dimensions_map[full_name] = short_name\n                        elif table_name:\n                            # Create full name with table prefix\n                            full_name = f\"{table_name}.{dimension_name}\"\n                            dimensions.append(full_name)\n                            dimensions_map[full_name] = dimension_name\n                        else:\n                            # If no table name, use just the dimension name\n                            dimensions.append(dimension_name)\n                            dimensions_map[dimension_name] = dimension_name\n                else:\n                    # Fallback: use first column\n                    dimensions_raw = dimensions_df.iloc[:, 0].dropna().unique().tolist()\n                    for dim in dimensions_raw:\n                        dimensions.append(str(dim))\n                        dimensions_map[str(dim)] = str(dim)\n            else:\n                st.warning(\"‚ö†Ô∏è No dimensions found in semantic view\")\n            \n            # Fallback values if nothing found\n            if not metrics and not dimensions:\n                st.error(\"‚ùå Could not retrieve metrics or dimensions. Using fallback values.\")\n                st.info(\"üí° Make sure the semantic view exists and is accessible\")\n                metrics = [\"HR_EMPLOYEE_FACT.TOTAL_EMPLOYEES\", \"HR_EMPLOYEE_FACT.AVG_SALARY\", \n                          \"HR_EMPLOYEE_FACT.TOTAL_SALARY_COST\", \"HR_EMPLOYEE_FACT.ATTRITION_COUNT\"]\n                dimensions = [\"DEPARTMENT_DIM.DEPARTMENT_NAME\", \"JOB_DIM.JOB_TITLE\", \n                            \"LOCATION_DIM.LOCATION_NAME\", \"EMPLOYEE_DIM.EMPLOYEE_NAME\"]\n                # Create mappings for fallback\n                for m in metrics:\n                    metrics_map[m] = m.split('.')[-1] if '.' in m else m\n                for d in dimensions:\n                    dimensions_map[d] = d.split('.')[-1] if '.' in d else d\n            elif not metrics:\n                st.warning(\"‚ö†Ô∏è No metrics found, using fallback\")\n                metrics = [\"HR_EMPLOYEE_FACT.TOTAL_EMPLOYEES\", \"HR_EMPLOYEE_FACT.AVG_SALARY\", \n                          \"HR_EMPLOYEE_FACT.TOTAL_SALARY_COST\"]\n                for m in metrics:\n                    metrics_map[m] = m.split('.')[-1] if '.' in m else m\n            elif not dimensions:\n                st.warning(\"‚ö†Ô∏è No dimensions found, using fallback\")\n                dimensions = [\"DEPARTMENT_DIM.DEPARTMENT_NAME\", \"JOB_DIM.JOB_TITLE\", \n                            \"LOCATION_DIM.LOCATION_NAME\"]\n                for d in dimensions:\n                    dimensions_map[d] = d.split('.')[-1] if '.' in d else d\n            \n        except Exception as e:\n            st.error(f\"‚ùå Error fetching metrics/dimensions: {str(e)}\")\n            st.info(\"üí° Using fallback values. Make sure the semantic view exists and is accessible.\")\n            # Fallback values\n            metrics = [\"HR_EMPLOYEE_FACT.TOTAL_EMPLOYEES\", \"HR_EMPLOYEE_FACT.AVG_SALARY\", \n                      \"HR_EMPLOYEE_FACT.TOTAL_SALARY_COST\", \"HR_EMPLOYEE_FACT.ATTRITION_COUNT\"]\n            dimensions = [\"DEPARTMENT_DIM.DEPARTMENT_NAME\", \"JOB_DIM.JOB_TITLE\", \n                        \"LOCATION_DIM.LOCATION_NAME\", \"EMPLOYEE_DIM.EMPLOYEE_NAME\"]\n            # Create mappings for fallback\n            for m in metrics:\n                metrics_map[m] = m.split('.')[-1] if '.' in m else m\n            for d in dimensions:\n                dimensions_map[d] = d.split('.')[-1] if '.' in d else d\n            import traceback\n            with st.expander(\"üîç Error Details\"):\n                st.code(traceback.format_exc(), language='python')\n        \n        # Remove duplicates while preserving order\n        metrics = list(dict.fromkeys(metrics))\n        dimensions = list(dict.fromkeys(dimensions))\n        \n        return metrics, dimensions, metrics_map, dimensions_map\n\n    try:\n        metrics, dimensions, metrics_map, dimensions_map = get_options()\n        \n        if not metrics or not dimensions:\n            st.error(\"‚ùå Could not load metrics or dimensions. Please check the semantic view.\")\n            return\n        \n        # Create two columns for the dropdowns\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            selected_metric_full = st.selectbox(\n                \"üìä Select Metric:\",\n                metrics,\n                help=\"Choose a metric to visualize\",\n                index=0 if metrics else None\n            )\n        \n        with col2:\n            selected_dimension_full = st.selectbox(\n                \"üìè Select Dimension:\",\n                dimensions,\n                help=\"Choose a dimension to group by\",\n                index=0 if dimensions else None\n            )\n        \n        if selected_metric_full and selected_dimension_full:\n            # Get short names for ORDER BY (without table prefix)\n            selected_metric_short = metrics_map.get(selected_metric_full, selected_metric_full.split('.')[-1] if '.' in selected_metric_full else selected_metric_full)\n            selected_dimension_short = dimensions_map.get(selected_dimension_full, selected_dimension_full.split('.')[-1] if '.' in selected_dimension_full else selected_dimension_full)\n            \n            # Configuration section\n            st.markdown(\"---\")\n            st.subheader(\"‚öôÔ∏è Visualization Configuration\")\n            \n            col_config1, col_config2, col_config3, col_config4 = st.columns(4)\n            \n            with col_config1:\n                limit_rows = st.number_input(\n                    \"üìä Number of Rows:\",\n                    min_value=1,\n                    max_value=1000,\n                    value=10,\n                    step=1,\n                    help=\"Limit the number of rows returned\"\n                )\n            \n            with col_config2:\n                viz_type = st.selectbox(\n                    \"üìà Visualization Type:\",\n                    [\"Table\", \"Vertical Bar\", \"Horizontal Bar\", \"Line\", \"Pie\"],\n                    index=1,  # Default to Vertical Bar\n                    help=\"Choose the chart type\"\n                )\n            \n            with col_config3:\n                sort_by = st.selectbox(\n                    \"üîÄ Sort By:\",\n                    [\"Metric\", \"Dimension\"],\n                    index=0,  # Default to Metric\n                    help=\"Choose which column to sort by\"\n                )\n            \n            with col_config4:\n                sort_direction = st.selectbox(\n                    \"‚¨ÜÔ∏è Sort Direction:\",\n                    [\"DESC\", \"ASC\"],\n                    index=0,  # Default to DESC\n                    help=\"Choose sort direction\"\n                )\n            \n            # Determine sort column\n            if sort_by == \"Metric\":\n                sort_column = selected_metric_short\n            else:\n                sort_column = selected_dimension_short\n            \n            # Generate semantic SQL using SEMANTIC_VIEW() function\n            # Use full names (with table prefix) inside SEMANTIC_VIEW()\n            # Use short names (without prefix) in ORDER BY outside SEMANTIC_VIEW()\n            query_sql = f\"\"\"SELECT * FROM SEMANTIC_VIEW(\n    {SEMANTIC_VIEW_FULL_NAME}\n    DIMENSIONS {selected_dimension_full}\n    METRICS {selected_metric_full}\n) ORDER BY {sort_column} {sort_direction} LIMIT {limit_rows}\"\"\"\n            \n            # Show the generated SQL in an expander\n            with st.expander(\"üìã View Generated Semantic SQL\"):\n                st.code(query_sql, language='sql')\n            \n            # Execute the query and create visualization\n            try:\n                with st.spinner(\"üîÑ Executing query and creating visualization...\"):\n                    try:\n                        result = session.sql(query_sql).collect()\n                    except Exception as sql_error:\n                        # If full name doesn't work, try with just the view name\n                        if \"SEMANTIC_VIEW\" in str(sql_error).upper() or \"syntax\" in str(sql_error).lower():\n                            st.info(\"üí° Trying with view name only (without schema qualification)...\")\n                            fallback_query = f\"\"\"SELECT * FROM SEMANTIC_VIEW(\n    {SEMANTIC_VIEW_NAME}\n    DIMENSIONS {selected_dimension_full}\n    METRICS {selected_metric_full}\n) ORDER BY {sort_column} {sort_direction} LIMIT {limit_rows}\"\"\"\n                            result = session.sql(fallback_query).collect()\n                            query_sql = fallback_query  # Update the query shown\n                        else:\n                            raise sql_error\n                \n                if result and len(result) > 0:\n                    # Convert to DataFrame\n                    df = pd.DataFrame([dict(row.asDict()) for row in result])\n                    \n                    # Clean column names\n                    df.columns = [col.strip() for col in df.columns]\n                    \n                    # Ensure we have numeric data for the metric\n                    if len(df.columns) >= 2:\n                        # Try to convert metric column to numeric\n                        metric_col = df.columns[1]\n                        df[metric_col] = pd.to_numeric(df[metric_col], errors='coerce')\n                    \n                    # Determine which columns to use\n                    x_col = df.columns[0]\n                    y_col = df.columns[1] if len(df.columns) > 1 else selected_metric_short\n                    \n                    # Explicitly sort the dataframe to maintain SQL sort order\n                    # This ensures Plotly respects the sort order\n                    sort_col_in_df = None\n                    if sort_by == \"Metric\":\n                        sort_col_in_df = y_col\n                    else:\n                        sort_col_in_df = x_col\n                    \n                    # Sort dataframe to match SQL ORDER BY\n                    ascending = (sort_direction == \"ASC\")\n                    df = df.sort_values(by=sort_col_in_df, ascending=ascending).reset_index(drop=True)\n                    \n                    metric_name = selected_metric_short.replace('_', ' ').title()\n                    dimension_name = selected_dimension_short.replace('_', ' ').title()\n                    \n                    # Create visualization based on selected type\n                    if viz_type == \"Table\":\n                        # Show table directly\n                        st.dataframe(df, use_container_width=True)\n                    else:\n                        # Create chart based on type\n                        if viz_type == \"Vertical Bar\":\n                            # Create category order to preserve dataframe sort order\n                            category_order = df[x_col].tolist()\n                            fig = px.bar(\n                                df, \n                                x=x_col, \n                                y=y_col,\n                                title=f'{metric_name} by {dimension_name}',\n                                labels={\n                                    x_col: dimension_name,\n                                    y_col: metric_name\n                                },\n                                color=y_col,\n                                color_continuous_scale='Blues',\n                                category_orders={x_col: category_order}\n                            )\n                            fig.update_layout(\n                                showlegend=False,\n                                height=500,\n                                xaxis_tickangle=-45,\n                                hovermode='x unified',\n                                xaxis={'categoryorder': 'array', 'categoryarray': category_order}\n                            )\n                        \n                        elif viz_type == \"Horizontal Bar\":\n                            # For horizontal bars, preserve y-axis (category) order\n                            category_order = df[x_col].tolist()\n                            fig = px.bar(\n                                df, \n                                x=y_col,\n                                y=x_col,\n                                orientation='h',\n                                title=f'{metric_name} by {dimension_name}',\n                                labels={\n                                    x_col: dimension_name,\n                                    y_col: metric_name\n                                },\n                                color=y_col,\n                                color_continuous_scale='Blues',\n                                category_orders={x_col: category_order}\n                            )\n                            fig.update_layout(\n                                showlegend=False,\n                                height=max(400, len(df) * 30),  # Dynamic height based on rows\n                                hovermode='y unified',\n                                yaxis={'categoryorder': 'array', 'categoryarray': category_order}\n                            )\n                        \n                        elif viz_type == \"Line\":\n                            # Preserve x-axis order for line charts\n                            category_order = df[x_col].tolist()\n                            fig = px.line(\n                                df, \n                                x=x_col, \n                                y=y_col,\n                                title=f'{metric_name} by {dimension_name}',\n                                labels={\n                                    x_col: dimension_name,\n                                    y_col: metric_name\n                                },\n                                markers=True,\n                                category_orders={x_col: category_order}\n                            )\n                            fig.update_layout(\n                                height=500,\n                                xaxis_tickangle=-45,\n                                hovermode='x unified',\n                                xaxis={'categoryorder': 'array', 'categoryarray': category_order}\n                            )\n                        \n                        elif viz_type == \"Pie\":\n                            fig = px.pie(\n                                df,\n                                values=y_col,\n                                names=x_col,\n                                title=f'{metric_name} by {dimension_name}'\n                            )\n                            fig.update_layout(\n                                height=500,\n                                showlegend=True\n                            )\n                            fig.update_traces(textposition='inside', textinfo='percent+label')\n                        \n                        st.plotly_chart(fig, use_container_width=True)\n                    \n                    # Show data table in expander (always available)\n                    with st.expander(\"üìä View Data Table\"):\n                        st.dataframe(df, use_container_width=True)\n                    \n                    # Show query execution info\n                    with st.expander(\"üîç Query Execution Details\"):\n                        st.code(query_sql, language='sql')\n                        st.write(f\"**Rows returned:** {len(df)}\")\n                        st.write(f\"**Columns:** {', '.join(df.columns)}\")\n                        if len(df.columns) >= 2:\n                            st.write(f\"**Metric range:** {df[y_col].min():,.2f} to {df[y_col].max():,.2f}\")\n                    \n                    st.success(f\"‚úÖ Successfully visualized {len(df)} data points!\")\n                    \n                else:\n                    st.warning(\"‚ö†Ô∏è No data returned from the semantic view query\")\n                    st.info(\"üí° Try selecting different metrics or dimensions\")\n                    \n            except Exception as e:\n                st.error(f\"‚ùå Error executing query: {str(e)}\")\n                st.info(\"üí° Troubleshooting tips:\")\n                st.info(\"1. Make sure the semantic view exists and is accessible\")\n                st.info(\"2. Verify you have proper permissions to query the semantic view\")\n                st.info(\"3. Check that the metric and dimension names are correct\")\n                st.info(\"4. Try the SQL query manually in a SQL cell to debug\")\n                import traceback\n                with st.expander(\"üîç Error Details\"):\n                    st.code(traceback.format_exc(), language='python')\n    \n    except Exception as e:\n        st.error(f\"‚ùå Error loading options: {str(e)}\")\n        st.info(\"üí° Make sure the semantic view was created successfully\")\n        import traceback\n        with st.expander(\"üîç Error Details\"):\n            st.code(traceback.format_exc(), language='python')\n\n# Run the Streamlit app\nif __name__ == \"__main__\":\n    main()\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8339f39b-245e-4d60-ac7c-68dcf4f5973f",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": "### „Åì„ÅÆ„Çª„É´„Åß„ÅØ„ÄÅCortex Analyst REST API „ÇíÂà©Áî®„Åó„Å¶„ÄÅ„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„Å´Ëá™ÁÑ∂Ë®ÄË™û„ÅßË≥™Âïè„Åß„Åç„Çã Streamlit „Ç¢„Éó„É™„ÇíÂÆöÁæ©„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n- ÊåáÂÆö„Çπ„Ç≠„Éº„ÉûÂÜÖ„ÅÆ„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„Çí `SHOW SEMANTIC VIEWS` „ÅßÂàóÊåô„Åó„ÄÅ„Éâ„É≠„ÉÉ„Éó„ÉÄ„Ç¶„É≥„Åã„ÇâÈÅ∏Êäû\n- Ë≥™ÂïèÊñáÔºà‰æã: \"What are the top 5 departments by average salary?\"Ôºâ„ÇíÂÖ•Âäõ„Åó„ÄÅ„ÄåAnswer!„Äç„Éú„Çø„É≥„ÅßÈÄÅ‰ø°\n- `_snowflake.send_snow_api_request(\"/api/v2/cortex/analyst/message\", ...)` „ÇíÂëº„Å≥Âá∫„Åó„ÄÅÈÅ∏Êäû„Åó„Åü„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„ÇíÊñáËÑà„Å´ SQL „ÇíËá™ÂãïÁîüÊàê\n- ÂøúÁ≠î„É°„ÉÉ„Çª„Éº„Ç∏„Åã„Çâ `type: \"sql\"` „ÅÆ„Éñ„É≠„ÉÉ„ÇØ„ÇíÊäΩÂá∫„Åó„Å¶ÂÆüË°å„Åó„ÄÅÁµêÊûú„Çí DataFrame „Å®„Åó„Å¶Ë°®Á§∫\n- ÂøÖË¶Å„Å´Âøú„Åò„Å¶„ÄÅËß£Èáà„ÉÜ„Ç≠„Çπ„Éà„ÉªË≠¶Âëä„Éª„É¨„Çπ„Éù„É≥„Çπ„É°„Çø„Éá„Éº„Çø„ÉªÂÆåÂÖ®„Å™„É¨„Çπ„Éù„É≥„Çπ JSON „ÇíÁ¢∫Ë™ç„Åß„Åç„Çã„Åü„ÇÅ„ÄÅËá™ÁÑ∂Ë®ÄË™û„ÇØ„Ç®„É™„ÅÆÊåôÂãï„ÇíÂèØË¶ñÂåñ„Åó„Å™„Åå„ÇâÊ§úË®º„Åß„Åç„Åæ„Åô„ÄÇ"
  },
  {
   "cell_type": "code",
   "id": "40d0e8b9-20ea-4f6b-9070-47aa7335608d",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "# Natural Language Query Interface for Semantic Views\n# Streamlit App for Snowflake Notebooks\n# Uses Cortex Analyst REST API\n# \n# Usage in Snowflake Notebook:\n# 1. Make sure you're in a Snowflake notebook (not local Streamlit)\n# 2. The 'session' variable should be automatically available\n# 3. Paste this code into a Streamlit cell\n# 4. Select a semantic view from the dropdown\n# 5. Type your natural language question\n# 6. Click \"Answer!\" to execute\n#\n# Note: If session is not available, ensure you're running in a Snowflake notebook environment.\n# The session variable is created automatically when you run a SQL cell in a Snowflake notebook.\n\nimport streamlit as st\nimport pandas as pd\nimport json\nimport time\n\n# Try to import _snowflake (available in Snowflake notebooks)\ntry:\n    import _snowflake  # For interacting with Snowflake-specific APIs\n    SNOWFLAKE_API_AVAILABLE = True\nexcept ImportError:\n    SNOWFLAKE_API_AVAILABLE = False\n    _snowflake = None\n\n# Schema configuration - adjust if needed\nDEFAULT_SCHEMA = \"SV_VHOL_DB.VHOL_SCHEMA\"\n\ndef make_authenticated_request_via_session(session, url, method=\"POST\", json_data=None, headers=None):\n    \"\"\"\n    Attempt to make an HTTP request using the session's connection\n    This bypasses the need for explicit OAuth token extraction\n    \"\"\"\n    try:\n        # Try to get the connection object\n        conn = None\n        if hasattr(session, '_conn'):\n            conn = session._conn\n        elif hasattr(session, 'connection'):\n            conn = session.connection\n        \n        if not conn:\n            return None\n        \n        # Try different methods to make HTTP requests through the connection\n        # Method 1: Check if connection has an HTTP client or request method\n        if hasattr(conn, '_request') or hasattr(conn, 'request'):\n            request_method = getattr(conn, '_request', None) or getattr(conn, 'request', None)\n            if request_method:\n                try:\n                    # Try to make the request\n                    response = request_method(url, method=method, json=json_data, headers=headers)\n                    return response\n                except:\n                    pass\n        \n        # Method 2: Check if there's an HTTP client or session object\n        if hasattr(conn, '_http') or hasattr(conn, 'http') or hasattr(conn, '_session') or hasattr(conn, 'session'):\n            http_client = (getattr(conn, '_http', None) or \n                          getattr(conn, 'http', None) or\n                          getattr(conn, '_session', None) or\n                          getattr(conn, 'session', None))\n            if http_client:\n                try:\n                    if method == \"POST\":\n                        response = http_client.post(url, json=json_data, headers=headers)\n                    else:\n                        response = http_client.request(method, url, json=json_data, headers=headers)\n                    return response\n                except:\n                    pass\n        \n    except Exception:\n        pass\n    \n    return None\n\ndef generate_oauth_token_from_session(session, account, region):\n    \"\"\"\n    Attempt to generate an OAuth token using the current session\n    This uses Snowflake's OAuth API to create a token for REST API calls\n    \"\"\"\n    try:\n        # Try to use Snowflake's OAuth token generation\n        # Note: SYSTEM$GENERATE_OAUTH_TOKEN might not be available\n        try:\n            token_result = session.sql(\"SELECT SYSTEM$GENERATE_OAUTH_TOKEN() as token\").collect()\n            if token_result and len(token_result) > 0:\n                token = token_result[0].get('TOKEN')\n                if token:\n                    return token\n        except:\n            # SYSTEM$GENERATE_OAUTH_TOKEN might not be available\n            pass\n        \n    except Exception as e:\n        # Silently fail\n        pass\n    \n    return None\n\ndef get_auth_token(session):\n    \"\"\"Try to extract authentication token from Snowflake session\"\"\"\n    auth_token = None\n    \n    def _check_object_for_token(obj, depth=0, max_depth=3):\n        \"\"\"Recursively search an object for token-like values\"\"\"\n        if depth > max_depth or obj is None:\n            return None\n        \n        # Check direct token attributes\n        token_attrs = ['_token', 'token', '_master_token', 'master_token', '_session_token', \n                      'session_token', 'access_token', '_access_token', 'bearer_token', '_bearer_token']\n        for attr in token_attrs:\n            if hasattr(obj, attr):\n                try:\n                    value = getattr(obj, attr)\n                    if value and isinstance(value, str) and len(value) > 20:  # Tokens are usually long strings\n                        return value\n                except:\n                    pass\n        \n        # Check if it's a dict-like object\n        if hasattr(obj, '__dict__'):\n            for key, value in obj.__dict__.items():\n                if 'token' in key.lower() and isinstance(value, str) and len(value) > 20:\n                    return value\n                # Recursively check nested objects (but limit depth)\n                if depth < max_depth and isinstance(value, object) and not isinstance(value, (str, int, float, bool)):\n                    result = _check_object_for_token(value, depth + 1, max_depth)\n                    if result:\n                        return result\n        \n        return None\n    \n    try:\n        # Try to get from session's connection\n        conn = None\n        \n        # Method 1: Try session._conn (Snowpark)\n        if hasattr(session, '_conn'):\n            conn = session._conn\n        # Method 2: Try session.connection (alternative attribute name)\n        elif hasattr(session, 'connection'):\n            conn = session.connection\n        # Method 3: Try session._connection (another variant)\n        elif hasattr(session, '_connection'):\n            conn = session._connection\n        \n        if conn:\n            # Method A: Try REST client token (for Python connector connections)\n            if hasattr(conn, '_rest'):\n                rest_client = conn._rest\n                # Try direct attributes first\n                for token_attr in ['_token', 'token', '_master_token', 'master_token', '_session_token']:\n                    if hasattr(rest_client, token_attr):\n                        try:\n                            token_value = getattr(rest_client, token_attr)\n                            if token_value and isinstance(token_value, str) and len(token_value) > 20:\n                                auth_token = token_value\n                                break\n                        except:\n                            pass\n                \n                # Try recursive search if direct access failed\n                if not auth_token:\n                    auth_token = _check_object_for_token(rest_client, max_depth=2)\n                \n                # Try token manager if available\n                if not auth_token and hasattr(rest_client, '_token_manager'):\n                    token_manager = rest_client._token_manager\n                    auth_token = _check_object_for_token(token_manager, max_depth=2)\n            \n            # Method A2: For ServerConnection (Snowflake notebooks), try different attributes\n            # ServerConnection might have token stored differently\n            if not auth_token:\n                # Try connection-level token attributes\n                auth_token = _check_object_for_token(conn, max_depth=3)\n            \n            # Method A3: Try to get from connection's internal state\n            if not auth_token:\n                # Check for session token or authentication state\n                internal_attrs = ['_session_token', '_auth_token', '_token', 'token', \n                                 '_session', '_authenticator', '_login_manager']\n                for attr in internal_attrs:\n                    if hasattr(conn, attr):\n                        try:\n                            value = getattr(conn, attr)\n                            if isinstance(value, str) and len(value) > 20:\n                                auth_token = value\n                                break\n                            elif hasattr(value, '__dict__'):\n                                # If it's an object, search it recursively\n                                token = _check_object_for_token(value, max_depth=2)\n                                if token:\n                                    auth_token = token\n                                    break\n                        except:\n                            pass\n            \n            # Method B: Try connection-level token attributes (recursive)\n            if not auth_token:\n                auth_token = _check_object_for_token(conn, max_depth=3)\n            \n            # Method C: Try from connection's authentication handler\n            if not auth_token:\n                auth_attrs = ['_authenticate', '_auth', 'authenticate', '_auth_handler', 'auth_handler']\n                for auth_attr in auth_attrs:\n                    if hasattr(conn, auth_attr):\n                        try:\n                            auth_handler = getattr(conn, auth_attr)\n                            auth_token = _check_object_for_token(auth_handler, max_depth=2)\n                            if auth_token:\n                                break\n                        except:\n                            pass\n            \n            # Method D: Try to get from connection's headers/cookies\n            if not auth_token and hasattr(conn, '_rest'):\n                rest_client = conn._rest\n                # Check if there's a headers dict with authorization\n                header_attrs = ['_headers', 'headers', '_request_headers', 'request_headers']\n                for header_attr in header_attrs:\n                    if hasattr(rest_client, header_attr):\n                        try:\n                            headers = getattr(rest_client, header_attr)\n                            if isinstance(headers, dict):\n                                auth_header = headers.get('Authorization') or headers.get('authorization')\n                                if auth_header and isinstance(auth_header, str):\n                                    if auth_header.startswith('Bearer '):\n                                        auth_token = auth_header[7:]  # Remove 'Bearer ' prefix\n                                    else:\n                                        auth_token = auth_header\n                                    if auth_token:\n                                        break\n                        except:\n                            pass\n    \n    except Exception as e:\n        # Silently fail - we'll handle missing token in the UI\n        pass\n    \n    return auth_token\n\ndef main():\n    st.title(\"üí¨ Natural Language Query for Semantic Views\")\n    st.markdown(\"Ask questions in plain English about your semantic view data\")\n    st.markdown(\"*Using [Cortex Analyst REST API](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst/rest-api)*\")\n    \n    # Check if session is available (Snowflake notebook context)\n    # In Snowflake notebooks, session is typically available as a global variable\n    if 'session' not in globals():\n        st.error(\"‚ùå Snowflake session not available. Please run this in a Snowflake notebook.\")\n        st.info(\"üí° Make sure you're running this in a Snowflake notebook with `session` available\")\n        return\n    \n    # Get account and region info early - cache it for the session\n    @st.cache_data\n    def get_account_info():\n        \"\"\"Get account and region from the current Snowflake session\"\"\"\n        try:\n            account_info = session.sql(\"SELECT CURRENT_ACCOUNT() as account, CURRENT_REGION() as region\").collect()\n            if account_info and len(account_info) > 0:\n                account = account_info[0]['ACCOUNT']\n                region = account_info[0]['REGION']\n                return account, region\n        except Exception:\n            pass\n        return None, None\n    \n    # Pre-populate account and region first (needed for token generation)\n    account, region = get_account_info()\n    \n    # Get token early - cache it for the session\n    @st.cache_data\n    def get_cached_token(account_val, region_val):\n        \"\"\"Get auth token from session - cached, tries extraction then generation\"\"\"\n        # First try to extract existing token\n        token = get_auth_token(session)\n        \n        # If extraction failed and we have account/region, try generating one\n        if not token and account_val and region_val:\n            try:\n                token = generate_oauth_token_from_session(session, account_val, region_val)\n            except:\n                pass\n        \n        return token\n    \n    # Check if _snowflake API is available (required for authentication)\n    if account and region:\n        if not SNOWFLAKE_API_AVAILABLE:\n            st.error(\"‚ö†Ô∏è `_snowflake` module not available. This app requires running in a Snowflake notebook.\")\n            st.info(\"üí° The `_snowflake` module provides automatic authentication for REST API calls.\")\n            return\n    else:\n        st.warning(\"‚ö†Ô∏è Could not retrieve account information. Some features may not work.\")\n    \n    # Get available semantic views in the schema\n    @st.cache_data\n    def get_semantic_views(schema_name): # ÊâÄÊúõ„ÅÆ„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„ÇíÂèñÂæó„Åô„ÇãÈñ¢Êï∞\n        \"\"\"Get list of available semantic views in the schema\"\"\"\n        try:\n            # Handle schema name (could be \"DATABASE.SCHEMA\" or just \"SCHEMA\")\n            if '.' in schema_name:\n                database, schema = schema_name.split('.', 1)\n                show_sql = f\"SHOW SEMANTIC VIEWS IN SCHEMA {database}.{schema}\"\n            else:\n                # Try to use current database context\n                show_sql = f\"SHOW SEMANTIC VIEWS IN SCHEMA {schema_name}\"\n            \n            result = session.sql(show_sql).collect()\n            \n            if result and len(result) > 0:\n                # Convert to DataFrame\n                views_df = pd.DataFrame([dict(row.asDict()) for row in result])\n                \n                # Try to find the name column\n                name_col = None\n                for col in ['name', 'semantic_view_name', 'view_name', 'NAME', 'SEMANTIC_VIEW_NAME']:\n                    if col in views_df.columns:\n                        name_col = col\n                        break\n                \n                if name_col:\n                    views = views_df[name_col].dropna().unique().tolist()\n                else:\n                    # Fallback: use first column\n                    views = views_df.iloc[:, 0].dropna().unique().tolist()\n                \n                # Create full qualified names\n                full_names = []\n                for view in views:\n                    full_name = f\"{schema_name}.{view}\" if '.' not in view else view\n                    full_names.append(full_name)\n                \n                return full_names, views_df\n            else:\n                return [], pd.DataFrame()\n                \n        except Exception as e:\n            st.error(f\"‚ùå Error fetching semantic views: {str(e)}\")\n            return [], pd.DataFrame()\n    \n    # Schema selection\n    schema_input = st.text_input(\n        \"üìÅ Schema:\",\n        value=DEFAULT_SCHEMA,\n        help=\"Enter the schema path (e.g., DATABASE.SCHEMA)\"\n    )\n    \n    # Get semantic views\n    with st.spinner(\"üîç Loading semantic views...\"):\n        semantic_views, views_df = get_semantic_views(schema_input)\n    \n    if not semantic_views:\n        st.warning(f\"‚ö†Ô∏è No semantic views found in {schema_input}\")\n        st.info(\"üí° Make sure the schema name is correct and contains semantic views\")\n        \n        # Show debug info if available\n        if not views_df.empty:\n            with st.expander(\"üîç Debug: SHOW SEMANTIC VIEWS Result\"):\n                st.dataframe(views_df)\n        return\n    \n    # Semantic view selection\n    selected_view = st.selectbox(\n        \"üìä Select Semantic View:\",\n        semantic_views,\n        help=\"Choose a semantic view to query\",\n        index=0 if semantic_views else None\n    )\n    \n    if selected_view:\n        st.markdown(\"---\")\n        \n        # Natural language question input\n        st.subheader(\"üí¨ Ask Your Question\")\n        question = st.text_area(\n            \"Enter your question:\",\n            height=100,\n            placeholder=\"e.g., What are the top 5 departments by average salary?\",\n            help=\"Type your question in natural language\"\n        )\n        \n        # Answer button\n        col1, col2 = st.columns([1, 4])\n        with col1:\n            answer_button = st.button(\"üöÄ Answer!\", type=\"primary\", use_container_width=True)\n        \n        if answer_button and question:\n            if not question.strip():\n                st.warning(\"‚ö†Ô∏è Please enter a question\")\n            else:\n                # Generate SQL from natural language question using Cortex Analyst REST API\n                generated_sql = None  # Initialize outside try block\n                \n                try:\n                    with st.spinner(\"ü§ñ Generating SQL from your question...\"):\n                        # Use Snowflake's built-in API request method (no token needed!)\n                        if not SNOWFLAKE_API_AVAILABLE:\n                            st.error(\"‚ùå `_snowflake` module not available. Make sure you're running this in a Snowflake notebook.\")\n                            st.info(\"üí° The `_snowflake` module is automatically available in Snowflake notebooks.\")\n                            return\n                        \n                        # Build request body for Cortex Analyst API\n                        # According to Snowflake Labs example: https://github.com/Snowflake-Labs/sfguide-getting-started-with-cortex-analyst\n                        # Note: API requires exactly one of: semantic_model, semantic_model_file, or semantic_view\n                        request_body = {\n                            \"messages\": [\n                                {\n                                    \"role\": \"user\",\n                                    \"content\": [\n                                        {\n                                            \"type\": \"text\",\n                                            \"text\": question\n                                        }\n                                    ]\n                                }\n                            ],\n                            \"semantic_view\": selected_view # Cortex Analyst„ÅÆRESTAPI„ÅÆBody„Å´ÊåáÂÆö„Åó„Åü„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ„Éì„É•„Éº„ÇíÊ∏°„Åô\n                        }\n                        \n                        # Use Snowflake's built-in API request method\n                        # This automatically handles authentication - no token needed!\n                        API_ENDPOINT = \"/api/v2/cortex/analyst/message\"\n                        API_TIMEOUT = 50000  # in milliseconds\n                        \n                        resp = _snowflake.send_snow_api_request(\n                            \"POST\",  # method\n                            API_ENDPOINT,  # path\n                            {},  # headers (empty - auth is handled automatically)\n                            {},  # params\n                            request_body,  # body\n                            None,  # request_guid\n                            API_TIMEOUT,  # timeout in milliseconds\n                        )\n                        \n                        # Parse response\n                        # Content is a string with serialized JSON object\n                        parsed_content = json.loads(resp[\"content\"])\n                        \n                        # Check if the response is successful\n                        if resp[\"status\"] >= 400:\n                            # Error response\n                            error_msg = f\"\"\"\nüö® An Analyst API error has occurred üö®\n\n* response code: `{resp['status']}`\n* request-id: `{parsed_content.get('request_id', 'N/A')}`\n* error code: `{parsed_content.get('error_code', 'N/A')}`\n\nMessage:\n\n{parsed_content.get('message', 'Unknown error')}\n\n                            \"\"\"\n                            st.error(error_msg)\n                            generated_sql = None\n                        else:\n                            # Success - extract response data\n                            response_data = parsed_content\n                            \n                            # Extract SQL from response\n                            # Response structure: message.content[] with type \"sql\" containing \"statement\"\n                            text_response = None\n                            \n                            if 'message' in response_data and 'content' in response_data['message']:\n                                for content_block in response_data['message']['content']:\n                                    if content_block.get('type') == 'sql':\n                                        generated_sql = content_block.get('statement', '')\n                                    elif content_block.get('type') == 'text':\n                                        text_response = content_block.get('text', '')\n                            \n                            # Show text interpretation if available\n                            if text_response:\n                                with st.expander(\"üìù Interpretation\", expanded=False):\n                                    st.write(text_response)\n                            \n                            # Show warnings if any\n                            if 'warnings' in response_data and response_data['warnings']:\n                                for warning in response_data['warnings']:\n                                    st.warning(f\"‚ö†Ô∏è {warning.get('message', 'Warning')}\")\n                            \n                            if generated_sql:\n                                # Show generated SQL\n                                with st.expander(\"üîç Generated SQL Query\", expanded=False):\n                                    st.code(generated_sql, language='sql')\n                                \n                                # Show response metadata if available\n                                if 'response_metadata' in response_data:\n                                    with st.expander(\"üìä Response Metadata\", expanded=False):\n                                        st.json(response_data['response_metadata'])\n                            else:\n                                # Check if suggestions were provided\n                                suggestions_found = False\n                                if 'message' in response_data and 'content' in response_data['message']:\n                                    for content_block in response_data['message']['content']:\n                                        if content_block.get('type') == 'suggestions':\n                                            st.info(\"üí° Your question might be ambiguous. Here are some suggestions:\")\n                                            suggestions = content_block.get('suggestions', [])\n                                            for i, suggestion in enumerate(suggestions, 1):\n                                                st.write(f\"{i}. {suggestion}\")\n                                            suggestions_found = True\n                                \n                                if not suggestions_found:\n                                    st.error(\"‚ùå No SQL generated. Check the response for details.\")\n                                    with st.expander(\"üîç Full Response\"):\n                                        st.json(response_data)\n                                    generated_sql = None  # Ensure it's None if no SQL generated\n                        \n                        # Execute the query if SQL was generated\n                        if generated_sql:\n                            with st.spinner(\"üîÑ Executing query...\"):\n                                try:\n                                    result = session.sql(generated_sql).collect()\n                                    \n                                    if result and len(result) > 0:\n                                        # Convert to DataFrame\n                                        df = pd.DataFrame([dict(row.asDict()) for row in result])\n                                        \n                                        # Display results\n                                        st.subheader(\"üìä Results\")\n                                        st.dataframe(df, use_container_width=True)\n                                        \n                                        # Show summary\n                                        st.success(f\"‚úÖ Query executed successfully! Returned {len(df)} rows.\")\n                                        \n                                        # Show query details\n                                        with st.expander(\"üìã Query Details\"):\n                                            st.code(generated_sql, language='sql')\n                                            st.write(f\"**Rows returned:** {len(df)}\")\n                                            st.write(f\"**Columns:** {', '.join(df.columns)}\")\n                                        \n                                    else:\n                                        st.info(\"‚ÑπÔ∏è Query executed but returned no results.\")\n                                        \n                                except Exception as e:\n                                    st.error(f\"‚ùå Error executing query: {str(e)}\")\n                                    st.info(\"üí° The generated SQL might need adjustment. Check the generated SQL above.\")\n                                    import traceback\n                                    with st.expander(\"üîç Error Details\"):\n                                        st.code(traceback.format_exc(), language='python')\n                        \n                        else:\n                            st.error(\"‚ùå Could not generate SQL from Cortex Analyst API\")\n                            st.info(\"üí° Check the API response above for details.\")\n                    \n                except Exception as e:\n                    st.error(f\"‚ùå Error generating SQL: {str(e)}\")\n                    st.info(\"üí° Make sure you're running in a Snowflake notebook and that Cortex Analyst is available in your account.\")\n                    import traceback\n                    with st.expander(\"üîç Error Details\"):\n                        st.code(traceback.format_exc(), language='python')\n    \n    # Show available semantic views info\n    with st.expander(\"‚ÑπÔ∏è About This App\"):\n        st.markdown(\"\"\"\n        **How to use:**\n        1. Select a semantic view from the dropdown\n        2. Type your question in natural language\n        3. Click \"Answer!\" to generate and execute the query\n        \n        **Example questions:**\n        - 2025Âπ¥„ÅÆÂèñÂºïÈáëÈ°çÂêàË®à„Åß‰∏ä‰Ωç10‰ª∂„ÅÆÈÉ®ÈñÄ„ÅØ„Å©„Åì„Åß„Åô„ÅãÔºü\n        - 2024Âπ¥„ÅÆ„Éà„É©„É≥„Ç∂„ÇØ„Ç∑„Éß„É≥‰ª∂Êï∞ÂêàË®à„Åß‰∏ä‰Ωç10‰ª∂„ÅÆÈÉ®ÈñÄ„ÅØ„Å©„Åì„Åß„Åô„ÅãÔºü\n        - ÊâøË™çÊ∏à„Åø„Éà„É©„É≥„Ç∂„ÇØ„Ç∑„Éß„É≥„ÅÆÂèñÂºïÈáëÈ°çÂêàË®à„Åß‰∏ä‰Ωç10‰ª∂„ÅÆÈÉ®ÈñÄ„ÅØ„Å©„Åì„Åß„Åô„ÅãÔºü\n        \n        **Note:** This app uses the [Cortex Analyst REST API](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst/rest-api) \n        to generate SQL from natural language questions. The API automatically understands your semantic view \n        structure and generates appropriate queries.\n        \n        **Authentication:** The app attempts to automatically retrieve your authentication token from the session.\n        If that fails, you can manually enter an OAuth token when prompted.\n        \"\"\")\n\n# Run the Streamlit app\nif __name__ == \"__main__\":\n    main()\n\n\n",
   "execution_count": null
  }
 ]
}